{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f6a44d536d84731a82ac6871085dbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f67da5c894bd4941b4c2b693dd067636",
              "IPY_MODEL_fb108adb8d00410e980b7d1e66b44262",
              "IPY_MODEL_df2ec3c39e924d2394020b525b598536"
            ],
            "layout": "IPY_MODEL_bc2bd476a44d427fb03d86ac038a2d69"
          }
        },
        "f67da5c894bd4941b4c2b693dd067636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b77f1d736361484498345e29c41962b7",
            "placeholder": "​",
            "style": "IPY_MODEL_0b8a886270204320ab131200ffd3878f",
            "value": "100%"
          }
        },
        "fb108adb8d00410e980b7d1e66b44262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0cb6780a3324eaab6415a32a45d4f69",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53485f63532b4813a25e7d5ba9fa77e1",
            "value": 102530333
          }
        },
        "df2ec3c39e924d2394020b525b598536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39ac5c507a6549ee96e4aaf67908ddd1",
            "placeholder": "​",
            "style": "IPY_MODEL_12f2787c151d43d2a15c3aacafaaac97",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 111MB/s]"
          }
        },
        "bc2bd476a44d427fb03d86ac038a2d69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b77f1d736361484498345e29c41962b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8a886270204320ab131200ffd3878f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0cb6780a3324eaab6415a32a45d4f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53485f63532b4813a25e7d5ba9fa77e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39ac5c507a6549ee96e4aaf67908ddd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12f2787c151d43d2a15c3aacafaaac97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Установка необходимых библиотек"
      ],
      "metadata": {
        "id": "PFl6kX-JeKdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE1RDE13yXsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f662d5af-1a83-4858-e98e-497c23c64397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-dotenv==0.19.0 in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Requirement already satisfied: tqdm==4.62.2 in /usr/local/lib/python3.7/dist-packages (4.62.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.7/dist-packages (7.0.0)\n",
            "Requirement already satisfied: matplotlib==3.4.3 in /usr/local/lib/python3.7/dist-packages (3.4.3)\n",
            "Requirement already satisfied: opencv-python==4.6.0.66 in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: opencv-python-headless==4.6.0.66 in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.3) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.3) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.4.3) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.4.3) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn==0.24.2 in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision==0.13.1 in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.10)\n",
            "Requirement already satisfied: segmentation-models-pytorch==0.2.0 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: albumentations==1.0.3 in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.13.1) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.13.1) (2.23.0)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.0) (0.7.4)\n",
            "Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.0) (0.4.12)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.0) (0.6.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (6.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (4.6.0.66)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (4.62.2)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (2.5.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.9.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (3.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (21.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.13.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.13.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.13.1) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.13.1) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv==0.19.0 tqdm==4.62.2 numpy Pillow==7.0.0 matplotlib==3.4.3 opencv-python==4.6.0.66 opencv-python-headless==4.6.0.66 matplotlib\n",
        "!pip install scikit-learn==0.24.2 torch==1.12.1 torchvision==0.13.1 pytorch-ignite segmentation-models-pytorch==0.2.0 albumentations==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o ~/plan-dataset.zip \"https://lodmedia.hb.bizmrg.com/cases/868821/masterclass.zip\"\n",
        "!unzip ~/plan-dataset.zip -d ~/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRhiSQ5zM8nO",
        "outputId": "53b64f66-6165-4abc-ae48-56bdde71edb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 22.3M  100 22.3M    0     0   631k      0  0:00:36  0:00:36 --:--:--  645k\n",
            "Archive:  /root/plan-dataset.zip\n",
            "   creating: /root/train/object_detection/\n",
            "  inflating: /root/train/object_detection/09f87310-ba17-4e30-98b1-dd02d48ed805.json  \n",
            "  inflating: /root/train/object_detection/7b533338-17ad-45c6-8cdf-b716f7669226.json  \n",
            "  inflating: /root/train/object_detection/6dd39e76-7f4f-46aa-86dd-457dc94823b6.json  \n",
            "  inflating: /root/train/object_detection/4646d4d5-c50c-43b1-881f-db96555977f2.json  \n",
            "  inflating: /root/train/object_detection/bc2894fa-1e4c-4eed-a573-d1c46cf819b5.json  \n",
            "  inflating: /root/train/object_detection/4e3d768c-6e73-433e-b2ae-a3eddf698b5d.json  \n",
            "  inflating: /root/train/object_detection/891d4884-7c1c-4ec9-a9d8-16184fb8140e.json  \n",
            "  inflating: /root/train/object_detection/744f177d-3bf9-4829-b208-4e3fcd86c9c2.json  \n",
            "  inflating: /root/train/object_detection/0f59c83f-c387-403b-8d7c-b017027b98ea.json  \n",
            "  inflating: /root/train/object_detection/3c8a77b6-9890-49a8-a5f7-2ee79cd65b13.json  \n",
            "  inflating: /root/train/object_detection/6606ed1b-6f40-4286-a283-bf9b62e6049c.json  \n",
            "  inflating: /root/train/object_detection/566b561a-6506-4c6c-b9f9-d15f841d05c7.json  \n",
            "  inflating: /root/train/object_detection/2eb34d1e-de5c-4c78-b47a-096db09ff40f.json  \n",
            "  inflating: /root/train/object_detection/5ee7e84c-4394-4c00-abcb-33f71c48d0f5.json  \n",
            "  inflating: /root/train/object_detection/8d4ddc53-c1cc-48a9-a4d1-37235c4c6f1d.json  \n",
            "  inflating: /root/train/object_detection/f6e25dcd-2185-4a32-9894-fa11ba244dab.json  \n",
            "  inflating: /root/train/object_detection/ba6fe5db-8464-4bd4-9790-76ce86ef2975.json  \n",
            "  inflating: /root/train/object_detection/59454aa4-3405-44e0-95bf-3547037e5c0f.json  \n",
            "  inflating: /root/train/object_detection/6c36599b-9f2c-411a-8db0-df12c7d70a12.json  \n",
            "  inflating: /root/train/object_detection/a77cce46-4f0f-4977-9257-80ad7f1932b0.json  \n",
            "  inflating: /root/train/object_detection/bb61d03c-689b-4ff4-9151-02944b95388c.json  \n",
            "  inflating: /root/train/object_detection/b7c1c688-61e0-41ab-8e44-8735557cbe58.json  \n",
            "  inflating: /root/train/object_detection/4dc9d171-939a-426a-86cd-1c09b06afc0a.json  \n",
            "  inflating: /root/train/object_detection/88fc7652-7273-40fc-8888-280d1a9124b2.json  \n",
            "  inflating: /root/train/object_detection/d9dc0e4a-a4b3-43f3-9526-606fdadc4b78.json  \n",
            "  inflating: /root/train/object_detection/cc0cb75b-6b4e-4b84-9411-0169599efb41.json  \n",
            "  inflating: /root/train/object_detection/6cce1a07-3378-48cd-9afc-69ffc2f7cb5e.json  \n",
            "  inflating: /root/train/object_detection/790d29f9-6e5a-4394-b866-02d3f06ea52d.json  \n",
            "  inflating: /root/train/object_detection/8a9cfe4c-b68b-425b-a17a-69ea22f10bc0.json  \n",
            "  inflating: /root/train/object_detection/95c256f5-6479-467a-8ad6-3b45f3d19e15.json  \n",
            "  inflating: /root/train/object_detection/16e3ca99-4be0-4930-82ec-c08006c733be.json  \n",
            "  inflating: /root/train/object_detection/2eee92cb-1176-42fa-9217-0372d60b4612.json  \n",
            "  inflating: /root/train/object_detection/346a3aaa-1298-455e-9470-944616be4d1f.json  \n",
            "  inflating: /root/train/object_detection/36015dce-f73b-495c-85b3-887e3ddbfb7b.json  \n",
            "  inflating: /root/train/object_detection/5e63ae25-b9c6-4b7b-b4fc-00bfe66c5a81.json  \n",
            "  inflating: /root/train/object_detection/8a54d6ad-85cf-4b77-a747-23c0b72e2a59.json  \n",
            "  inflating: /root/train/object_detection/102a121e-689b-46ea-b891-a703779b29c6.json  \n",
            "  inflating: /root/train/object_detection/6db2ed4b-27f9-47c5-a464-9492b943ae74.json  \n",
            "  inflating: /root/train/object_detection/491ce6f1-7e54-4081-a835-3d47bbb09779.json  \n",
            "  inflating: /root/train/object_detection/2c425cbe-4cfd-4ccc-a038-a024a11e8ed4.json  \n",
            "  inflating: /root/train/object_detection/08b76fd6-41ee-4661-aa21-3245ff821449.json  \n",
            "  inflating: /root/train/object_detection/5c0086b8-0372-404a-b62f-a114e8745287.json  \n",
            "  inflating: /root/train/object_detection/b0a762cd-3df4-4041-bc58-455b15c0ae75.json  \n",
            "  inflating: /root/train/object_detection/3578cfb0-51a8-46b1-a1e9-aa9b1b398697.json  \n",
            "  inflating: /root/train/object_detection/7b4c6b2c-1e60-4be0-97c9-57dbadba9df6.json  \n",
            "  inflating: /root/train/object_detection/3752cf67-d14f-4913-83d8-cabc51d8c9c8.json  \n",
            "  inflating: /root/train/object_detection/3b8326d3-52d8-4eb4-acfb-c7f7bdae1242.json  \n",
            "  inflating: /root/train/object_detection/06eb85b5-44c1-45da-8e14-5a5edba6b48a.json  \n",
            "  inflating: /root/train/object_detection/00b20126-42c7-4546-86e0-1bef375bd121.json  \n",
            "  inflating: /root/train/object_detection/02b99381-aadc-4080-9a90-afc9c08596c6.json  \n",
            "  inflating: /root/train/object_detection/9b7f5494-765c-497e-802e-5c9f8a495e00.json  \n",
            "  inflating: /root/train/object_detection/fea58b20-90fd-48f9-bde8-f6489bc15b04.json  \n",
            "  inflating: /root/train/object_detection/2ec075ed-7c6f-4a5b-a2d0-7082c3a166bb.json  \n",
            "  inflating: /root/train/object_detection/72a70bf5-480f-42ad-9550-7deb20b8165d.json  \n",
            "  inflating: /root/train/object_detection/2585d308-235e-4998-80df-38db19bdb782.json  \n",
            "  inflating: /root/train/object_detection/b941bd6b-080b-47cd-b89c-6518d5e94de8.json  \n",
            "  inflating: /root/train/object_detection/7e7aa67f-bcf3-4ef0-86c4-abaed0bfb1f5.json  \n",
            "  inflating: /root/train/object_detection/12e3a68b-4624-46b5-b135-8593159e21e5.json  \n",
            "  inflating: /root/train/object_detection/415ceb0f-e9be-4b6b-91f7-89663beb9e6c.json  \n",
            "  inflating: /root/train/object_detection/02faf833-a693-403c-8748-b72c3b2e9d95.json  \n",
            "  inflating: /root/train/object_detection/bc7bc866-174f-4a47-b7bb-2af304f4005d.json  \n",
            "  inflating: /root/train/object_detection/1d6457c2-ccd7-496b-a972-6e41eb7d7250.json  \n",
            "  inflating: /root/train/object_detection/3ef2b648-f996-4c1a-a20c-130870e92f1f.json  \n",
            "  inflating: /root/train/object_detection/b7804135-c93f-4d2d-94a5-96be21464d0b.json  \n",
            "  inflating: /root/train/object_detection/2eaa2aa1-03cb-445d-b552-7c2f6e061946.json  \n",
            "  inflating: /root/train/object_detection/11ea50e3-0dc4-4cdb-9787-ed5c03e54c8f.json  \n",
            "  inflating: /root/train/object_detection/734e95b8-6ceb-4880-9ac3-2d72c8b90a74.json  \n",
            "  inflating: /root/train/object_detection/1b32ac36-b2f7-4db0-a28d-e6b53c56f20e.json  \n",
            "  inflating: /root/train/object_detection/7e000341-faa4-44d5-a213-45e9a7b74c74.json  \n",
            "  inflating: /root/train/object_detection/8748b3d2-e823-44dc-b8ac-73aec3931570.json  \n",
            "  inflating: /root/train/object_detection/3d3a3821-9eeb-4c5c-8e36-80eb3fedb4b2.json  \n",
            "  inflating: /root/train/object_detection/06e25cef-b8a7-46a8-ade0-daa62e06bd49.json  \n",
            "  inflating: /root/train/object_detection/8ea0c0b5-4478-4aca-9012-db7e041f2194.json  \n",
            "  inflating: /root/train/object_detection/8aea19b6-9d49-4099-8de7-64debc6a2a52.json  \n",
            "  inflating: /root/train/object_detection/124e9f2f-dec4-4b01-b744-5af1460b892f.json  \n",
            "  inflating: /root/train/object_detection/5b266074-9960-4424-bb62-d8faadc5dd1a.json  \n",
            "  inflating: /root/train/object_detection/6ef02353-cf46-40dc-bbb8-49c27fe45aa8.json  \n",
            "  inflating: /root/train/object_detection/6e505454-4c7d-475e-91ff-14a8e915a0ed.json  \n",
            "  inflating: /root/train/object_detection/ae6c91a0-858e-4b54-9198-997194d9bc2d.json  \n",
            "  inflating: /root/train/object_detection/c92fa0b2-a707-4ca0-8497-c15637904d26.json  \n",
            "  inflating: /root/train/object_detection/5b27dc5c-0cd8-4a02-8743-33d4fcd5086f.json  \n",
            "  inflating: /root/train/object_detection/55a8c840-6a26-4e9a-a24c-0e30574ace03.json  \n",
            "  inflating: /root/train/object_detection/4db16593-2ce6-4c1f-9985-b7a41aa0f2da.json  \n",
            "  inflating: /root/train/object_detection/525d5e05-bd2a-4b7e-ad39-cf0da7c7ff5d.json  \n",
            "  inflating: /root/train/object_detection/f5d32b09-aa93-4ca6-8ec7-4e56f79827e6.json  \n",
            "  inflating: /root/train/object_detection/17bfa9a3-326a-453a-8a62-00133d0b8def.json  \n",
            "  inflating: /root/train/object_detection/7ce98dbd-7a19-4093-a124-ac90ce6d1e51.json  \n",
            "  inflating: /root/train/object_detection/3d1c3664-5577-4234-81cb-44ceef435d7b.json  \n",
            "  inflating: /root/train/object_detection/2f0d9e79-d358-4725-a543-4c762655dd83.json  \n",
            "  inflating: /root/train/object_detection/635d63e2-ffb5-4f58-9306-7e0579b14cf6.json  \n",
            "  inflating: /root/train/object_detection/404ce340-2216-4771-bdd0-d4b45864f06a.json  \n",
            "  inflating: /root/train/object_detection/dc5815c6-9fc4-4579-a9e8-d7acf718195d.json  \n",
            "  inflating: /root/train/object_detection/13c76808-ff85-4ed4-b769-ead08efb3eb2.json  \n",
            "  inflating: /root/train/object_detection/c5ceafc8-1dd4-4ee1-ad4e-4ac203adc85c.json  \n",
            "  inflating: /root/train/object_detection/79d1a937-b5b4-4caa-a03e-5ba9d180cae6.json  \n",
            "  inflating: /root/train/object_detection/43896beb-804d-47d2-9eef-33ee98ad7c9b.json  \n",
            "  inflating: /root/train/object_detection/919074e6-0a32-43a0-b0fc-ad51ba477423.json  \n",
            "  inflating: /root/train/object_detection/2cd58e8b-f750-4e34-b0f2-c094b0c39e6b.json  \n",
            "  inflating: /root/train/object_detection/7b49559c-f54a-416e-8bb1-b62c17d517c4.json  \n",
            "  inflating: /root/train/object_detection/2d9babac-747e-45bb-b6b3-a92e1b8b1c0a.json  \n",
            "  inflating: /root/train/test.png    \n",
            "   creating: /root/train/wall/\n",
            "   creating: /root/train/wall/image/\n",
            "   creating: /root/train/wall/mask/\n",
            "  inflating: /root/train/wall/image/ae6c91a0-858e-4b54-9198-997194d9bc2d.png  \n",
            "  inflating: /root/train/wall/image/102a121e-689b-46ea-b891-a703779b29c6.png  \n",
            "  inflating: /root/train/wall/image/919074e6-0a32-43a0-b0fc-ad51ba477423.png  \n",
            "  inflating: /root/train/wall/image/55a8c840-6a26-4e9a-a24c-0e30574ace03.png  \n",
            "  inflating: /root/train/wall/image/346a3aaa-1298-455e-9470-944616be4d1f.png  \n",
            "  inflating: /root/train/wall/image/7e7aa67f-bcf3-4ef0-86c4-abaed0bfb1f5.png  \n",
            "  inflating: /root/train/wall/image/3d1c3664-5577-4234-81cb-44ceef435d7b.png  \n",
            "  inflating: /root/train/wall/image/525d5e05-bd2a-4b7e-ad39-cf0da7c7ff5d.png  \n",
            "  inflating: /root/train/wall/image/3d3a3821-9eeb-4c5c-8e36-80eb3fedb4b2.png  \n",
            "  inflating: /root/train/wall/image/6606ed1b-6f40-4286-a283-bf9b62e6049c.png  \n",
            "  inflating: /root/train/wall/image/b7c1c688-61e0-41ab-8e44-8735557cbe58.png  \n",
            "  inflating: /root/train/wall/image/00b20126-42c7-4546-86e0-1bef375bd121.png  \n",
            "  inflating: /root/train/wall/image/02b99381-aadc-4080-9a90-afc9c08596c6.png  \n",
            "  inflating: /root/train/wall/image/4646d4d5-c50c-43b1-881f-db96555977f2.png  \n",
            "  inflating: /root/train/wall/image/3752cf67-d14f-4913-83d8-cabc51d8c9c8.png  \n",
            "  inflating: /root/train/wall/image/415ceb0f-e9be-4b6b-91f7-89663beb9e6c.png  \n",
            "  inflating: /root/train/wall/image/2f0d9e79-d358-4725-a543-4c762655dd83.png  \n",
            "  inflating: /root/train/wall/image/c5ceafc8-1dd4-4ee1-ad4e-4ac203adc85c.png  \n",
            "  inflating: /root/train/wall/image/09f87310-ba17-4e30-98b1-dd02d48ed805.png  \n",
            "  inflating: /root/train/wall/image/dc5815c6-9fc4-4579-a9e8-d7acf718195d.png  \n",
            "  inflating: /root/train/wall/image/404ce340-2216-4771-bdd0-d4b45864f06a.png  \n",
            "  inflating: /root/train/wall/image/17bfa9a3-326a-453a-8a62-00133d0b8def.png  \n",
            "  inflating: /root/train/wall/image/cc0cb75b-6b4e-4b84-9411-0169599efb41.png  \n",
            "  inflating: /root/train/wall/image/95c256f5-6479-467a-8ad6-3b45f3d19e15.png  \n",
            "  inflating: /root/train/wall/image/7b4c6b2c-1e60-4be0-97c9-57dbadba9df6.png  \n",
            "  inflating: /root/train/wall/image/16e3ca99-4be0-4930-82ec-c08006c733be.png  \n",
            "  inflating: /root/train/wall/image/5ee7e84c-4394-4c00-abcb-33f71c48d0f5.png  \n",
            "  inflating: /root/train/wall/image/734e95b8-6ceb-4880-9ac3-2d72c8b90a74.png  \n",
            "  inflating: /root/train/wall/image/72a70bf5-480f-42ad-9550-7deb20b8165d.png  \n",
            "  inflating: /root/train/wall/image/2c425cbe-4cfd-4ccc-a038-a024a11e8ed4.png  \n",
            "  inflating: /root/train/wall/image/1b32ac36-b2f7-4db0-a28d-e6b53c56f20e.png  \n",
            "  inflating: /root/train/wall/image/2d9babac-747e-45bb-b6b3-a92e1b8b1c0a.png  \n",
            "  inflating: /root/train/wall/image/f6e25dcd-2185-4a32-9894-fa11ba244dab.png  \n",
            "  inflating: /root/train/wall/image/3b8326d3-52d8-4eb4-acfb-c7f7bdae1242.png  \n",
            "  inflating: /root/train/wall/image/bc2894fa-1e4c-4eed-a573-d1c46cf819b5.png  \n",
            "  inflating: /root/train/wall/image/566b561a-6506-4c6c-b9f9-d15f841d05c7.png  \n",
            "  inflating: /root/train/wall/image/4dc9d171-939a-426a-86cd-1c09b06afc0a.png  \n",
            "  inflating: /root/train/wall/image/8d4ddc53-c1cc-48a9-a4d1-37235c4c6f1d.png  \n",
            "  inflating: /root/train/wall/image/8aea19b6-9d49-4099-8de7-64debc6a2a52.png  \n",
            "  inflating: /root/train/wall/image/3c8a77b6-9890-49a8-a5f7-2ee79cd65b13.png  \n",
            "  inflating: /root/train/wall/image/4e3d768c-6e73-433e-b2ae-a3eddf698b5d.png  \n",
            "  inflating: /root/train/wall/image/bc7bc866-174f-4a47-b7bb-2af304f4005d.png  \n",
            "  inflating: /root/train/wall/image/79d1a937-b5b4-4caa-a03e-5ba9d180cae6.png  \n",
            "  inflating: /root/train/wall/image/8a54d6ad-85cf-4b77-a747-23c0b72e2a59.png  \n",
            "  inflating: /root/train/wall/image/6ef02353-cf46-40dc-bbb8-49c27fe45aa8.png  \n",
            "  inflating: /root/train/wall/image/2585d308-235e-4998-80df-38db19bdb782.png  \n",
            "  inflating: /root/train/wall/image/1d6457c2-ccd7-496b-a972-6e41eb7d7250.png  \n",
            "  inflating: /root/train/wall/image/c92fa0b2-a707-4ca0-8497-c15637904d26.png  \n",
            "  inflating: /root/train/wall/image/7b533338-17ad-45c6-8cdf-b716f7669226.png  \n",
            "  inflating: /root/train/wall/image/bb61d03c-689b-4ff4-9151-02944b95388c.png  \n",
            "  inflating: /root/train/wall/image/491ce6f1-7e54-4081-a835-3d47bbb09779.png  \n",
            "  inflating: /root/train/wall/image/8748b3d2-e823-44dc-b8ac-73aec3931570.png  \n",
            "  inflating: /root/train/wall/image/d9dc0e4a-a4b3-43f3-9526-606fdadc4b78.png  \n",
            "  inflating: /root/train/wall/image/fea58b20-90fd-48f9-bde8-f6489bc15b04.png  \n",
            "  inflating: /root/train/wall/image/88fc7652-7273-40fc-8888-280d1a9124b2.png  \n",
            "  inflating: /root/train/wall/image/635d63e2-ffb5-4f58-9306-7e0579b14cf6.png  \n",
            "  inflating: /root/train/wall/image/3578cfb0-51a8-46b1-a1e9-aa9b1b398697.png  \n",
            "  inflating: /root/train/wall/image/43896beb-804d-47d2-9eef-33ee98ad7c9b.png  \n",
            "  inflating: /root/train/wall/image/02faf833-a693-403c-8748-b72c3b2e9d95.png  \n",
            "  inflating: /root/train/wall/image/06e25cef-b8a7-46a8-ade0-daa62e06bd49.png  \n",
            "  inflating: /root/train/wall/image/2eee92cb-1176-42fa-9217-0372d60b4612.png  \n",
            "  inflating: /root/train/wall/image/11ea50e3-0dc4-4cdb-9787-ed5c03e54c8f.png  \n",
            "  inflating: /root/train/wall/image/06eb85b5-44c1-45da-8e14-5a5edba6b48a.png  \n",
            "  inflating: /root/train/wall/image/b941bd6b-080b-47cd-b89c-6518d5e94de8.png  \n",
            "  inflating: /root/train/wall/image/2eaa2aa1-03cb-445d-b552-7c2f6e061946.png  \n",
            "  inflating: /root/train/wall/image/6e505454-4c7d-475e-91ff-14a8e915a0ed.png  \n",
            "  inflating: /root/train/wall/image/08b76fd6-41ee-4661-aa21-3245ff821449.png  \n",
            "  inflating: /root/train/wall/image/891d4884-7c1c-4ec9-a9d8-16184fb8140e.png  \n",
            "  inflating: /root/train/wall/image/13c76808-ff85-4ed4-b769-ead08efb3eb2.png  \n",
            "  inflating: /root/train/wall/image/b7804135-c93f-4d2d-94a5-96be21464d0b.png  \n",
            "  inflating: /root/train/wall/image/9b7f5494-765c-497e-802e-5c9f8a495e00.png  \n",
            "  inflating: /root/train/wall/image/124e9f2f-dec4-4b01-b744-5af1460b892f.png  \n",
            "  inflating: /root/train/wall/image/8ea0c0b5-4478-4aca-9012-db7e041f2194.png  \n",
            "  inflating: /root/train/wall/image/b0a762cd-3df4-4041-bc58-455b15c0ae75.png  \n",
            "  inflating: /root/train/wall/image/5c0086b8-0372-404a-b62f-a114e8745287.png  \n",
            "  inflating: /root/train/wall/image/5b27dc5c-0cd8-4a02-8743-33d4fcd5086f.png  \n",
            "  inflating: /root/train/wall/image/6c36599b-9f2c-411a-8db0-df12c7d70a12.png  \n",
            "  inflating: /root/train/wall/image/6db2ed4b-27f9-47c5-a464-9492b943ae74.png  \n",
            "  inflating: /root/train/wall/image/6cce1a07-3378-48cd-9afc-69ffc2f7cb5e.png  \n",
            "  inflating: /root/train/wall/image/a77cce46-4f0f-4977-9257-80ad7f1932b0.png  \n",
            "  inflating: /root/train/wall/image/36015dce-f73b-495c-85b3-887e3ddbfb7b.png  \n",
            "  inflating: /root/train/wall/image/3ef2b648-f996-4c1a-a20c-130870e92f1f.png  \n",
            "  inflating: /root/train/wall/image/5e63ae25-b9c6-4b7b-b4fc-00bfe66c5a81.png  \n",
            "  inflating: /root/train/wall/image/2ec075ed-7c6f-4a5b-a2d0-7082c3a166bb.png  \n",
            "  inflating: /root/train/wall/image/0f59c83f-c387-403b-8d7c-b017027b98ea.png  \n",
            "  inflating: /root/train/wall/image/744f177d-3bf9-4829-b208-4e3fcd86c9c2.png  \n",
            "  inflating: /root/train/wall/image/59454aa4-3405-44e0-95bf-3547037e5c0f.png  \n",
            "  inflating: /root/train/wall/image/5b266074-9960-4424-bb62-d8faadc5dd1a.png  \n",
            "  inflating: /root/train/wall/image/f5d32b09-aa93-4ca6-8ec7-4e56f79827e6.png  \n",
            "  inflating: /root/train/wall/image/7e000341-faa4-44d5-a213-45e9a7b74c74.png  \n",
            "  inflating: /root/train/wall/image/12e3a68b-4624-46b5-b135-8593159e21e5.png  \n",
            "  inflating: /root/train/wall/image/7b49559c-f54a-416e-8bb1-b62c17d517c4.png  \n",
            "  inflating: /root/train/wall/image/2eb34d1e-de5c-4c78-b47a-096db09ff40f.png  \n",
            "  inflating: /root/train/wall/image/4db16593-2ce6-4c1f-9985-b7a41aa0f2da.png  \n",
            "  inflating: /root/train/wall/image/ba6fe5db-8464-4bd4-9790-76ce86ef2975.png  \n",
            "  inflating: /root/train/wall/image/6dd39e76-7f4f-46aa-86dd-457dc94823b6.png  \n",
            "  inflating: /root/train/wall/image/790d29f9-6e5a-4394-b866-02d3f06ea52d.png  \n",
            "  inflating: /root/train/wall/image/2cd58e8b-f750-4e34-b0f2-c094b0c39e6b.png  \n",
            "  inflating: /root/train/wall/image/7ce98dbd-7a19-4093-a124-ac90ce6d1e51.png  \n",
            "  inflating: /root/train/wall/image/8a9cfe4c-b68b-425b-a17a-69ea22f10bc0.png  \n",
            "  inflating: /root/train/wall/mask/ae6c91a0-858e-4b54-9198-997194d9bc2d.png  \n",
            "  inflating: /root/train/wall/mask/102a121e-689b-46ea-b891-a703779b29c6.png  \n",
            "  inflating: /root/train/wall/mask/919074e6-0a32-43a0-b0fc-ad51ba477423.png  \n",
            "  inflating: /root/train/wall/mask/55a8c840-6a26-4e9a-a24c-0e30574ace03.png  \n",
            "  inflating: /root/train/wall/mask/346a3aaa-1298-455e-9470-944616be4d1f.png  \n",
            "  inflating: /root/train/wall/mask/7e7aa67f-bcf3-4ef0-86c4-abaed0bfb1f5.png  \n",
            "  inflating: /root/train/wall/mask/3d1c3664-5577-4234-81cb-44ceef435d7b.png  \n",
            "  inflating: /root/train/wall/mask/525d5e05-bd2a-4b7e-ad39-cf0da7c7ff5d.png  \n",
            "  inflating: /root/train/wall/mask/3d3a3821-9eeb-4c5c-8e36-80eb3fedb4b2.png  \n",
            "  inflating: /root/train/wall/mask/6606ed1b-6f40-4286-a283-bf9b62e6049c.png  \n",
            "  inflating: /root/train/wall/mask/b7c1c688-61e0-41ab-8e44-8735557cbe58.png  \n",
            "  inflating: /root/train/wall/mask/00b20126-42c7-4546-86e0-1bef375bd121.png  \n",
            "  inflating: /root/train/wall/mask/02b99381-aadc-4080-9a90-afc9c08596c6.png  \n",
            "  inflating: /root/train/wall/mask/4646d4d5-c50c-43b1-881f-db96555977f2.png  \n",
            "  inflating: /root/train/wall/mask/3752cf67-d14f-4913-83d8-cabc51d8c9c8.png  \n",
            "  inflating: /root/train/wall/mask/415ceb0f-e9be-4b6b-91f7-89663beb9e6c.png  \n",
            "  inflating: /root/train/wall/mask/2f0d9e79-d358-4725-a543-4c762655dd83.png  \n",
            "  inflating: /root/train/wall/mask/c5ceafc8-1dd4-4ee1-ad4e-4ac203adc85c.png  \n",
            "  inflating: /root/train/wall/mask/09f87310-ba17-4e30-98b1-dd02d48ed805.png  \n",
            "  inflating: /root/train/wall/mask/dc5815c6-9fc4-4579-a9e8-d7acf718195d.png  \n",
            "  inflating: /root/train/wall/mask/404ce340-2216-4771-bdd0-d4b45864f06a.png  \n",
            "  inflating: /root/train/wall/mask/17bfa9a3-326a-453a-8a62-00133d0b8def.png  \n",
            "  inflating: /root/train/wall/mask/cc0cb75b-6b4e-4b84-9411-0169599efb41.png  \n",
            "  inflating: /root/train/wall/mask/95c256f5-6479-467a-8ad6-3b45f3d19e15.png  \n",
            "  inflating: /root/train/wall/mask/7b4c6b2c-1e60-4be0-97c9-57dbadba9df6.png  \n",
            "  inflating: /root/train/wall/mask/16e3ca99-4be0-4930-82ec-c08006c733be.png  \n",
            "  inflating: /root/train/wall/mask/5ee7e84c-4394-4c00-abcb-33f71c48d0f5.png  \n",
            "  inflating: /root/train/wall/mask/734e95b8-6ceb-4880-9ac3-2d72c8b90a74.png  \n",
            "  inflating: /root/train/wall/mask/72a70bf5-480f-42ad-9550-7deb20b8165d.png  \n",
            "  inflating: /root/train/wall/mask/2c425cbe-4cfd-4ccc-a038-a024a11e8ed4.png  \n",
            "  inflating: /root/train/wall/mask/1b32ac36-b2f7-4db0-a28d-e6b53c56f20e.png  \n",
            "  inflating: /root/train/wall/mask/2d9babac-747e-45bb-b6b3-a92e1b8b1c0a.png  \n",
            "  inflating: /root/train/wall/mask/f6e25dcd-2185-4a32-9894-fa11ba244dab.png  \n",
            "  inflating: /root/train/wall/mask/3b8326d3-52d8-4eb4-acfb-c7f7bdae1242.png  \n",
            "  inflating: /root/train/wall/mask/bc2894fa-1e4c-4eed-a573-d1c46cf819b5.png  \n",
            "  inflating: /root/train/wall/mask/566b561a-6506-4c6c-b9f9-d15f841d05c7.png  \n",
            "  inflating: /root/train/wall/mask/4dc9d171-939a-426a-86cd-1c09b06afc0a.png  \n",
            "  inflating: /root/train/wall/mask/8d4ddc53-c1cc-48a9-a4d1-37235c4c6f1d.png  \n",
            "  inflating: /root/train/wall/mask/8aea19b6-9d49-4099-8de7-64debc6a2a52.png  \n",
            "  inflating: /root/train/wall/mask/3c8a77b6-9890-49a8-a5f7-2ee79cd65b13.png  \n",
            "  inflating: /root/train/wall/mask/4e3d768c-6e73-433e-b2ae-a3eddf698b5d.png  \n",
            "  inflating: /root/train/wall/mask/bc7bc866-174f-4a47-b7bb-2af304f4005d.png  \n",
            "  inflating: /root/train/wall/mask/79d1a937-b5b4-4caa-a03e-5ba9d180cae6.png  \n",
            "  inflating: /root/train/wall/mask/8a54d6ad-85cf-4b77-a747-23c0b72e2a59.png  \n",
            "  inflating: /root/train/wall/mask/6ef02353-cf46-40dc-bbb8-49c27fe45aa8.png  \n",
            "  inflating: /root/train/wall/mask/2585d308-235e-4998-80df-38db19bdb782.png  \n",
            "  inflating: /root/train/wall/mask/1d6457c2-ccd7-496b-a972-6e41eb7d7250.png  \n",
            "  inflating: /root/train/wall/mask/c92fa0b2-a707-4ca0-8497-c15637904d26.png  \n",
            "  inflating: /root/train/wall/mask/7b533338-17ad-45c6-8cdf-b716f7669226.png  \n",
            "  inflating: /root/train/wall/mask/bb61d03c-689b-4ff4-9151-02944b95388c.png  \n",
            "  inflating: /root/train/wall/mask/491ce6f1-7e54-4081-a835-3d47bbb09779.png  \n",
            "  inflating: /root/train/wall/mask/8748b3d2-e823-44dc-b8ac-73aec3931570.png  \n",
            "  inflating: /root/train/wall/mask/d9dc0e4a-a4b3-43f3-9526-606fdadc4b78.png  \n",
            "  inflating: /root/train/wall/mask/fea58b20-90fd-48f9-bde8-f6489bc15b04.png  \n",
            "  inflating: /root/train/wall/mask/88fc7652-7273-40fc-8888-280d1a9124b2.png  \n",
            "  inflating: /root/train/wall/mask/635d63e2-ffb5-4f58-9306-7e0579b14cf6.png  \n",
            "  inflating: /root/train/wall/mask/3578cfb0-51a8-46b1-a1e9-aa9b1b398697.png  \n",
            "  inflating: /root/train/wall/mask/43896beb-804d-47d2-9eef-33ee98ad7c9b.png  \n",
            "  inflating: /root/train/wall/mask/02faf833-a693-403c-8748-b72c3b2e9d95.png  \n",
            "  inflating: /root/train/wall/mask/06e25cef-b8a7-46a8-ade0-daa62e06bd49.png  \n",
            "  inflating: /root/train/wall/mask/2eee92cb-1176-42fa-9217-0372d60b4612.png  \n",
            "  inflating: /root/train/wall/mask/11ea50e3-0dc4-4cdb-9787-ed5c03e54c8f.png  \n",
            "  inflating: /root/train/wall/mask/06eb85b5-44c1-45da-8e14-5a5edba6b48a.png  \n",
            "  inflating: /root/train/wall/mask/b941bd6b-080b-47cd-b89c-6518d5e94de8.png  \n",
            "  inflating: /root/train/wall/mask/2eaa2aa1-03cb-445d-b552-7c2f6e061946.png  \n",
            "  inflating: /root/train/wall/mask/6e505454-4c7d-475e-91ff-14a8e915a0ed.png  \n",
            "  inflating: /root/train/wall/mask/08b76fd6-41ee-4661-aa21-3245ff821449.png  \n",
            "  inflating: /root/train/wall/mask/891d4884-7c1c-4ec9-a9d8-16184fb8140e.png  \n",
            "  inflating: /root/train/wall/mask/13c76808-ff85-4ed4-b769-ead08efb3eb2.png  \n",
            "  inflating: /root/train/wall/mask/b7804135-c93f-4d2d-94a5-96be21464d0b.png  \n",
            "  inflating: /root/train/wall/mask/9b7f5494-765c-497e-802e-5c9f8a495e00.png  \n",
            "  inflating: /root/train/wall/mask/124e9f2f-dec4-4b01-b744-5af1460b892f.png  \n",
            "  inflating: /root/train/wall/mask/8ea0c0b5-4478-4aca-9012-db7e041f2194.png  \n",
            "  inflating: /root/train/wall/mask/b0a762cd-3df4-4041-bc58-455b15c0ae75.png  \n",
            "  inflating: /root/train/wall/mask/5c0086b8-0372-404a-b62f-a114e8745287.png  \n",
            "  inflating: /root/train/wall/mask/5b27dc5c-0cd8-4a02-8743-33d4fcd5086f.png  \n",
            "  inflating: /root/train/wall/mask/6c36599b-9f2c-411a-8db0-df12c7d70a12.png  \n",
            "  inflating: /root/train/wall/mask/6db2ed4b-27f9-47c5-a464-9492b943ae74.png  \n",
            "  inflating: /root/train/wall/mask/6cce1a07-3378-48cd-9afc-69ffc2f7cb5e.png  \n",
            "  inflating: /root/train/wall/mask/a77cce46-4f0f-4977-9257-80ad7f1932b0.png  \n",
            "  inflating: /root/train/wall/mask/36015dce-f73b-495c-85b3-887e3ddbfb7b.png  \n",
            "  inflating: /root/train/wall/mask/3ef2b648-f996-4c1a-a20c-130870e92f1f.png  \n",
            "  inflating: /root/train/wall/mask/5e63ae25-b9c6-4b7b-b4fc-00bfe66c5a81.png  \n",
            "  inflating: /root/train/wall/mask/2ec075ed-7c6f-4a5b-a2d0-7082c3a166bb.png  \n",
            "  inflating: /root/train/wall/mask/0f59c83f-c387-403b-8d7c-b017027b98ea.png  \n",
            "  inflating: /root/train/wall/mask/744f177d-3bf9-4829-b208-4e3fcd86c9c2.png  \n",
            "  inflating: /root/train/wall/mask/59454aa4-3405-44e0-95bf-3547037e5c0f.png  \n",
            "  inflating: /root/train/wall/mask/5b266074-9960-4424-bb62-d8faadc5dd1a.png  \n",
            "  inflating: /root/train/wall/mask/f5d32b09-aa93-4ca6-8ec7-4e56f79827e6.png  \n",
            "  inflating: /root/train/wall/mask/7e000341-faa4-44d5-a213-45e9a7b74c74.png  \n",
            "  inflating: /root/train/wall/mask/12e3a68b-4624-46b5-b135-8593159e21e5.png  \n",
            "  inflating: /root/train/wall/mask/7b49559c-f54a-416e-8bb1-b62c17d517c4.png  \n",
            "  inflating: /root/train/wall/mask/2eb34d1e-de5c-4c78-b47a-096db09ff40f.png  \n",
            "  inflating: /root/train/wall/mask/4db16593-2ce6-4c1f-9985-b7a41aa0f2da.png  \n",
            "  inflating: /root/train/wall/mask/ba6fe5db-8464-4bd4-9790-76ce86ef2975.png  \n",
            "  inflating: /root/train/wall/mask/6dd39e76-7f4f-46aa-86dd-457dc94823b6.png  \n",
            "  inflating: /root/train/wall/mask/790d29f9-6e5a-4394-b866-02d3f06ea52d.png  \n",
            "  inflating: /root/train/wall/mask/2cd58e8b-f750-4e34-b0f2-c094b0c39e6b.png  \n",
            "  inflating: /root/train/wall/mask/7ce98dbd-7a19-4093-a124-ac90ce6d1e51.png  \n",
            "  inflating: /root/train/wall/mask/8a9cfe4c-b68b-425b-a17a-69ea22f10bc0.png  \n",
            "   creating: /root/train/window/\n",
            "   creating: /root/train/window/image/\n",
            "   creating: /root/train/window/mask/\n",
            "  inflating: /root/train/window/image/ae6c91a0-858e-4b54-9198-997194d9bc2d.png  \n",
            "  inflating: /root/train/window/image/102a121e-689b-46ea-b891-a703779b29c6.png  \n",
            "  inflating: /root/train/window/image/919074e6-0a32-43a0-b0fc-ad51ba477423.png  \n",
            "  inflating: /root/train/window/image/55a8c840-6a26-4e9a-a24c-0e30574ace03.png  \n",
            "  inflating: /root/train/window/image/346a3aaa-1298-455e-9470-944616be4d1f.png  \n",
            "  inflating: /root/train/window/image/7e7aa67f-bcf3-4ef0-86c4-abaed0bfb1f5.png  \n",
            "  inflating: /root/train/window/image/3d1c3664-5577-4234-81cb-44ceef435d7b.png  \n",
            "  inflating: /root/train/window/image/525d5e05-bd2a-4b7e-ad39-cf0da7c7ff5d.png  \n",
            "  inflating: /root/train/window/image/3d3a3821-9eeb-4c5c-8e36-80eb3fedb4b2.png  \n",
            "  inflating: /root/train/window/image/6606ed1b-6f40-4286-a283-bf9b62e6049c.png  \n",
            "  inflating: /root/train/window/image/b7c1c688-61e0-41ab-8e44-8735557cbe58.png  \n",
            "  inflating: /root/train/window/image/00b20126-42c7-4546-86e0-1bef375bd121.png  \n",
            "  inflating: /root/train/window/image/02b99381-aadc-4080-9a90-afc9c08596c6.png  \n",
            "  inflating: /root/train/window/image/4646d4d5-c50c-43b1-881f-db96555977f2.png  \n",
            "  inflating: /root/train/window/image/3752cf67-d14f-4913-83d8-cabc51d8c9c8.png  \n",
            "  inflating: /root/train/window/image/415ceb0f-e9be-4b6b-91f7-89663beb9e6c.png  \n",
            "  inflating: /root/train/window/image/2f0d9e79-d358-4725-a543-4c762655dd83.png  \n",
            "  inflating: /root/train/window/image/c5ceafc8-1dd4-4ee1-ad4e-4ac203adc85c.png  \n",
            "  inflating: /root/train/window/image/09f87310-ba17-4e30-98b1-dd02d48ed805.png  \n",
            "  inflating: /root/train/window/image/dc5815c6-9fc4-4579-a9e8-d7acf718195d.png  \n",
            "  inflating: /root/train/window/image/404ce340-2216-4771-bdd0-d4b45864f06a.png  \n",
            "  inflating: /root/train/window/image/17bfa9a3-326a-453a-8a62-00133d0b8def.png  \n",
            "  inflating: /root/train/window/image/cc0cb75b-6b4e-4b84-9411-0169599efb41.png  \n",
            "  inflating: /root/train/window/image/95c256f5-6479-467a-8ad6-3b45f3d19e15.png  \n",
            "  inflating: /root/train/window/image/7b4c6b2c-1e60-4be0-97c9-57dbadba9df6.png  \n",
            "  inflating: /root/train/window/image/16e3ca99-4be0-4930-82ec-c08006c733be.png  \n",
            "  inflating: /root/train/window/image/5ee7e84c-4394-4c00-abcb-33f71c48d0f5.png  \n",
            "  inflating: /root/train/window/image/734e95b8-6ceb-4880-9ac3-2d72c8b90a74.png  \n",
            "  inflating: /root/train/window/image/72a70bf5-480f-42ad-9550-7deb20b8165d.png  \n",
            "  inflating: /root/train/window/image/2c425cbe-4cfd-4ccc-a038-a024a11e8ed4.png  \n",
            "  inflating: /root/train/window/image/1b32ac36-b2f7-4db0-a28d-e6b53c56f20e.png  \n",
            "  inflating: /root/train/window/image/2d9babac-747e-45bb-b6b3-a92e1b8b1c0a.png  \n",
            "  inflating: /root/train/window/image/f6e25dcd-2185-4a32-9894-fa11ba244dab.png  \n",
            "  inflating: /root/train/window/image/3b8326d3-52d8-4eb4-acfb-c7f7bdae1242.png  \n",
            "  inflating: /root/train/window/image/bc2894fa-1e4c-4eed-a573-d1c46cf819b5.png  \n",
            "  inflating: /root/train/window/image/566b561a-6506-4c6c-b9f9-d15f841d05c7.png  \n",
            "  inflating: /root/train/window/image/4dc9d171-939a-426a-86cd-1c09b06afc0a.png  \n",
            "  inflating: /root/train/window/image/8d4ddc53-c1cc-48a9-a4d1-37235c4c6f1d.png  \n",
            "  inflating: /root/train/window/image/8aea19b6-9d49-4099-8de7-64debc6a2a52.png  \n",
            "  inflating: /root/train/window/image/3c8a77b6-9890-49a8-a5f7-2ee79cd65b13.png  \n",
            "  inflating: /root/train/window/image/4e3d768c-6e73-433e-b2ae-a3eddf698b5d.png  \n",
            "  inflating: /root/train/window/image/bc7bc866-174f-4a47-b7bb-2af304f4005d.png  \n",
            "  inflating: /root/train/window/image/79d1a937-b5b4-4caa-a03e-5ba9d180cae6.png  \n",
            "  inflating: /root/train/window/image/8a54d6ad-85cf-4b77-a747-23c0b72e2a59.png  \n",
            "  inflating: /root/train/window/image/6ef02353-cf46-40dc-bbb8-49c27fe45aa8.png  \n",
            "  inflating: /root/train/window/image/2585d308-235e-4998-80df-38db19bdb782.png  \n",
            "  inflating: /root/train/window/image/1d6457c2-ccd7-496b-a972-6e41eb7d7250.png  \n",
            "  inflating: /root/train/window/image/c92fa0b2-a707-4ca0-8497-c15637904d26.png  \n",
            "  inflating: /root/train/window/image/7b533338-17ad-45c6-8cdf-b716f7669226.png  \n",
            "  inflating: /root/train/window/image/bb61d03c-689b-4ff4-9151-02944b95388c.png  \n",
            "  inflating: /root/train/window/image/491ce6f1-7e54-4081-a835-3d47bbb09779.png  \n",
            "  inflating: /root/train/window/image/8748b3d2-e823-44dc-b8ac-73aec3931570.png  \n",
            "  inflating: /root/train/window/image/d9dc0e4a-a4b3-43f3-9526-606fdadc4b78.png  \n",
            "  inflating: /root/train/window/image/fea58b20-90fd-48f9-bde8-f6489bc15b04.png  \n",
            "  inflating: /root/train/window/image/88fc7652-7273-40fc-8888-280d1a9124b2.png  \n",
            "  inflating: /root/train/window/image/635d63e2-ffb5-4f58-9306-7e0579b14cf6.png  \n",
            "  inflating: /root/train/window/image/3578cfb0-51a8-46b1-a1e9-aa9b1b398697.png  \n",
            "  inflating: /root/train/window/image/43896beb-804d-47d2-9eef-33ee98ad7c9b.png  \n",
            "  inflating: /root/train/window/image/02faf833-a693-403c-8748-b72c3b2e9d95.png  \n",
            "  inflating: /root/train/window/image/06e25cef-b8a7-46a8-ade0-daa62e06bd49.png  \n",
            "  inflating: /root/train/window/image/2eee92cb-1176-42fa-9217-0372d60b4612.png  \n",
            "  inflating: /root/train/window/image/11ea50e3-0dc4-4cdb-9787-ed5c03e54c8f.png  \n",
            "  inflating: /root/train/window/image/06eb85b5-44c1-45da-8e14-5a5edba6b48a.png  \n",
            "  inflating: /root/train/window/image/b941bd6b-080b-47cd-b89c-6518d5e94de8.png  \n",
            "  inflating: /root/train/window/image/2eaa2aa1-03cb-445d-b552-7c2f6e061946.png  \n",
            "  inflating: /root/train/window/image/6e505454-4c7d-475e-91ff-14a8e915a0ed.png  \n",
            "  inflating: /root/train/window/image/08b76fd6-41ee-4661-aa21-3245ff821449.png  \n",
            "  inflating: /root/train/window/image/891d4884-7c1c-4ec9-a9d8-16184fb8140e.png  \n",
            "  inflating: /root/train/window/image/13c76808-ff85-4ed4-b769-ead08efb3eb2.png  \n",
            "  inflating: /root/train/window/image/b7804135-c93f-4d2d-94a5-96be21464d0b.png  \n",
            "  inflating: /root/train/window/image/9b7f5494-765c-497e-802e-5c9f8a495e00.png  \n",
            "  inflating: /root/train/window/image/124e9f2f-dec4-4b01-b744-5af1460b892f.png  \n",
            "  inflating: /root/train/window/image/8ea0c0b5-4478-4aca-9012-db7e041f2194.png  \n",
            "  inflating: /root/train/window/image/b0a762cd-3df4-4041-bc58-455b15c0ae75.png  \n",
            "  inflating: /root/train/window/image/5c0086b8-0372-404a-b62f-a114e8745287.png  \n",
            "  inflating: /root/train/window/image/5b27dc5c-0cd8-4a02-8743-33d4fcd5086f.png  \n",
            "  inflating: /root/train/window/image/6c36599b-9f2c-411a-8db0-df12c7d70a12.png  \n",
            "  inflating: /root/train/window/image/6db2ed4b-27f9-47c5-a464-9492b943ae74.png  \n",
            "  inflating: /root/train/window/image/6cce1a07-3378-48cd-9afc-69ffc2f7cb5e.png  \n",
            "  inflating: /root/train/window/image/a77cce46-4f0f-4977-9257-80ad7f1932b0.png  \n",
            "  inflating: /root/train/window/image/36015dce-f73b-495c-85b3-887e3ddbfb7b.png  \n",
            "  inflating: /root/train/window/image/3ef2b648-f996-4c1a-a20c-130870e92f1f.png  \n",
            "  inflating: /root/train/window/image/5e63ae25-b9c6-4b7b-b4fc-00bfe66c5a81.png  \n",
            "  inflating: /root/train/window/image/2ec075ed-7c6f-4a5b-a2d0-7082c3a166bb.png  \n",
            "  inflating: /root/train/window/image/0f59c83f-c387-403b-8d7c-b017027b98ea.png  \n",
            "  inflating: /root/train/window/image/744f177d-3bf9-4829-b208-4e3fcd86c9c2.png  \n",
            "  inflating: /root/train/window/image/59454aa4-3405-44e0-95bf-3547037e5c0f.png  \n",
            "  inflating: /root/train/window/image/5b266074-9960-4424-bb62-d8faadc5dd1a.png  \n",
            "  inflating: /root/train/window/image/f5d32b09-aa93-4ca6-8ec7-4e56f79827e6.png  \n",
            "  inflating: /root/train/window/image/7e000341-faa4-44d5-a213-45e9a7b74c74.png  \n",
            "  inflating: /root/train/window/image/12e3a68b-4624-46b5-b135-8593159e21e5.png  \n",
            "  inflating: /root/train/window/image/7b49559c-f54a-416e-8bb1-b62c17d517c4.png  \n",
            "  inflating: /root/train/window/image/2eb34d1e-de5c-4c78-b47a-096db09ff40f.png  \n",
            "  inflating: /root/train/window/image/4db16593-2ce6-4c1f-9985-b7a41aa0f2da.png  \n",
            "  inflating: /root/train/window/image/ba6fe5db-8464-4bd4-9790-76ce86ef2975.png  \n",
            "  inflating: /root/train/window/image/6dd39e76-7f4f-46aa-86dd-457dc94823b6.png  \n",
            "  inflating: /root/train/window/image/790d29f9-6e5a-4394-b866-02d3f06ea52d.png  \n",
            "  inflating: /root/train/window/image/2cd58e8b-f750-4e34-b0f2-c094b0c39e6b.png  \n",
            "  inflating: /root/train/window/image/7ce98dbd-7a19-4093-a124-ac90ce6d1e51.png  \n",
            "  inflating: /root/train/window/image/8a9cfe4c-b68b-425b-a17a-69ea22f10bc0.png  \n",
            "  inflating: /root/train/window/mask/ae6c91a0-858e-4b54-9198-997194d9bc2d.png  \n",
            "  inflating: /root/train/window/mask/102a121e-689b-46ea-b891-a703779b29c6.png  \n",
            "  inflating: /root/train/window/mask/919074e6-0a32-43a0-b0fc-ad51ba477423.png  \n",
            "  inflating: /root/train/window/mask/55a8c840-6a26-4e9a-a24c-0e30574ace03.png  \n",
            "  inflating: /root/train/window/mask/346a3aaa-1298-455e-9470-944616be4d1f.png  \n",
            "  inflating: /root/train/window/mask/7e7aa67f-bcf3-4ef0-86c4-abaed0bfb1f5.png  \n",
            "  inflating: /root/train/window/mask/3d1c3664-5577-4234-81cb-44ceef435d7b.png  \n",
            "  inflating: /root/train/window/mask/525d5e05-bd2a-4b7e-ad39-cf0da7c7ff5d.png  \n",
            "  inflating: /root/train/window/mask/3d3a3821-9eeb-4c5c-8e36-80eb3fedb4b2.png  \n",
            "  inflating: /root/train/window/mask/6606ed1b-6f40-4286-a283-bf9b62e6049c.png  \n",
            "  inflating: /root/train/window/mask/b7c1c688-61e0-41ab-8e44-8735557cbe58.png  \n",
            "  inflating: /root/train/window/mask/00b20126-42c7-4546-86e0-1bef375bd121.png  \n",
            "  inflating: /root/train/window/mask/02b99381-aadc-4080-9a90-afc9c08596c6.png  \n",
            "  inflating: /root/train/window/mask/4646d4d5-c50c-43b1-881f-db96555977f2.png  \n",
            "  inflating: /root/train/window/mask/3752cf67-d14f-4913-83d8-cabc51d8c9c8.png  \n",
            "  inflating: /root/train/window/mask/415ceb0f-e9be-4b6b-91f7-89663beb9e6c.png  \n",
            "  inflating: /root/train/window/mask/2f0d9e79-d358-4725-a543-4c762655dd83.png  \n",
            "  inflating: /root/train/window/mask/c5ceafc8-1dd4-4ee1-ad4e-4ac203adc85c.png  \n",
            "  inflating: /root/train/window/mask/09f87310-ba17-4e30-98b1-dd02d48ed805.png  \n",
            "  inflating: /root/train/window/mask/dc5815c6-9fc4-4579-a9e8-d7acf718195d.png  \n",
            "  inflating: /root/train/window/mask/404ce340-2216-4771-bdd0-d4b45864f06a.png  \n",
            "  inflating: /root/train/window/mask/17bfa9a3-326a-453a-8a62-00133d0b8def.png  \n",
            "  inflating: /root/train/window/mask/cc0cb75b-6b4e-4b84-9411-0169599efb41.png  \n",
            "  inflating: /root/train/window/mask/95c256f5-6479-467a-8ad6-3b45f3d19e15.png  \n",
            "  inflating: /root/train/window/mask/7b4c6b2c-1e60-4be0-97c9-57dbadba9df6.png  \n",
            "  inflating: /root/train/window/mask/16e3ca99-4be0-4930-82ec-c08006c733be.png  \n",
            "  inflating: /root/train/window/mask/5ee7e84c-4394-4c00-abcb-33f71c48d0f5.png  \n",
            "  inflating: /root/train/window/mask/734e95b8-6ceb-4880-9ac3-2d72c8b90a74.png  \n",
            "  inflating: /root/train/window/mask/72a70bf5-480f-42ad-9550-7deb20b8165d.png  \n",
            "  inflating: /root/train/window/mask/2c425cbe-4cfd-4ccc-a038-a024a11e8ed4.png  \n",
            "  inflating: /root/train/window/mask/1b32ac36-b2f7-4db0-a28d-e6b53c56f20e.png  \n",
            "  inflating: /root/train/window/mask/2d9babac-747e-45bb-b6b3-a92e1b8b1c0a.png  \n",
            "  inflating: /root/train/window/mask/f6e25dcd-2185-4a32-9894-fa11ba244dab.png  \n",
            "  inflating: /root/train/window/mask/3b8326d3-52d8-4eb4-acfb-c7f7bdae1242.png  \n",
            "  inflating: /root/train/window/mask/bc2894fa-1e4c-4eed-a573-d1c46cf819b5.png  \n",
            "  inflating: /root/train/window/mask/566b561a-6506-4c6c-b9f9-d15f841d05c7.png  \n",
            "  inflating: /root/train/window/mask/4dc9d171-939a-426a-86cd-1c09b06afc0a.png  \n",
            "  inflating: /root/train/window/mask/8d4ddc53-c1cc-48a9-a4d1-37235c4c6f1d.png  \n",
            "  inflating: /root/train/window/mask/8aea19b6-9d49-4099-8de7-64debc6a2a52.png  \n",
            "  inflating: /root/train/window/mask/3c8a77b6-9890-49a8-a5f7-2ee79cd65b13.png  \n",
            "  inflating: /root/train/window/mask/4e3d768c-6e73-433e-b2ae-a3eddf698b5d.png  \n",
            "  inflating: /root/train/window/mask/bc7bc866-174f-4a47-b7bb-2af304f4005d.png  \n",
            "  inflating: /root/train/window/mask/79d1a937-b5b4-4caa-a03e-5ba9d180cae6.png  \n",
            "  inflating: /root/train/window/mask/8a54d6ad-85cf-4b77-a747-23c0b72e2a59.png  \n",
            "  inflating: /root/train/window/mask/6ef02353-cf46-40dc-bbb8-49c27fe45aa8.png  \n",
            "  inflating: /root/train/window/mask/2585d308-235e-4998-80df-38db19bdb782.png  \n",
            "  inflating: /root/train/window/mask/1d6457c2-ccd7-496b-a972-6e41eb7d7250.png  \n",
            "  inflating: /root/train/window/mask/c92fa0b2-a707-4ca0-8497-c15637904d26.png  \n",
            "  inflating: /root/train/window/mask/7b533338-17ad-45c6-8cdf-b716f7669226.png  \n",
            "  inflating: /root/train/window/mask/bb61d03c-689b-4ff4-9151-02944b95388c.png  \n",
            "  inflating: /root/train/window/mask/491ce6f1-7e54-4081-a835-3d47bbb09779.png  \n",
            "  inflating: /root/train/window/mask/8748b3d2-e823-44dc-b8ac-73aec3931570.png  \n",
            "  inflating: /root/train/window/mask/d9dc0e4a-a4b3-43f3-9526-606fdadc4b78.png  \n",
            "  inflating: /root/train/window/mask/fea58b20-90fd-48f9-bde8-f6489bc15b04.png  \n",
            "  inflating: /root/train/window/mask/88fc7652-7273-40fc-8888-280d1a9124b2.png  \n",
            "  inflating: /root/train/window/mask/635d63e2-ffb5-4f58-9306-7e0579b14cf6.png  \n",
            "  inflating: /root/train/window/mask/3578cfb0-51a8-46b1-a1e9-aa9b1b398697.png  \n",
            "  inflating: /root/train/window/mask/43896beb-804d-47d2-9eef-33ee98ad7c9b.png  \n",
            "  inflating: /root/train/window/mask/02faf833-a693-403c-8748-b72c3b2e9d95.png  \n",
            "  inflating: /root/train/window/mask/06e25cef-b8a7-46a8-ade0-daa62e06bd49.png  \n",
            "  inflating: /root/train/window/mask/2eee92cb-1176-42fa-9217-0372d60b4612.png  \n",
            "  inflating: /root/train/window/mask/11ea50e3-0dc4-4cdb-9787-ed5c03e54c8f.png  \n",
            "  inflating: /root/train/window/mask/06eb85b5-44c1-45da-8e14-5a5edba6b48a.png  \n",
            "  inflating: /root/train/window/mask/b941bd6b-080b-47cd-b89c-6518d5e94de8.png  \n",
            "  inflating: /root/train/window/mask/2eaa2aa1-03cb-445d-b552-7c2f6e061946.png  \n",
            "  inflating: /root/train/window/mask/6e505454-4c7d-475e-91ff-14a8e915a0ed.png  \n",
            "  inflating: /root/train/window/mask/08b76fd6-41ee-4661-aa21-3245ff821449.png  \n",
            "  inflating: /root/train/window/mask/891d4884-7c1c-4ec9-a9d8-16184fb8140e.png  \n",
            "  inflating: /root/train/window/mask/13c76808-ff85-4ed4-b769-ead08efb3eb2.png  \n",
            "  inflating: /root/train/window/mask/b7804135-c93f-4d2d-94a5-96be21464d0b.png  \n",
            "  inflating: /root/train/window/mask/9b7f5494-765c-497e-802e-5c9f8a495e00.png  \n",
            "  inflating: /root/train/window/mask/124e9f2f-dec4-4b01-b744-5af1460b892f.png  \n",
            "  inflating: /root/train/window/mask/8ea0c0b5-4478-4aca-9012-db7e041f2194.png  \n",
            "  inflating: /root/train/window/mask/b0a762cd-3df4-4041-bc58-455b15c0ae75.png  \n",
            "  inflating: /root/train/window/mask/5c0086b8-0372-404a-b62f-a114e8745287.png  \n",
            "  inflating: /root/train/window/mask/5b27dc5c-0cd8-4a02-8743-33d4fcd5086f.png  \n",
            "  inflating: /root/train/window/mask/6c36599b-9f2c-411a-8db0-df12c7d70a12.png  \n",
            "  inflating: /root/train/window/mask/6db2ed4b-27f9-47c5-a464-9492b943ae74.png  \n",
            "  inflating: /root/train/window/mask/6cce1a07-3378-48cd-9afc-69ffc2f7cb5e.png  \n",
            "  inflating: /root/train/window/mask/a77cce46-4f0f-4977-9257-80ad7f1932b0.png  \n",
            "  inflating: /root/train/window/mask/36015dce-f73b-495c-85b3-887e3ddbfb7b.png  \n",
            "  inflating: /root/train/window/mask/3ef2b648-f996-4c1a-a20c-130870e92f1f.png  \n",
            "  inflating: /root/train/window/mask/5e63ae25-b9c6-4b7b-b4fc-00bfe66c5a81.png  \n",
            "  inflating: /root/train/window/mask/2ec075ed-7c6f-4a5b-a2d0-7082c3a166bb.png  \n",
            "  inflating: /root/train/window/mask/0f59c83f-c387-403b-8d7c-b017027b98ea.png  \n",
            "  inflating: /root/train/window/mask/744f177d-3bf9-4829-b208-4e3fcd86c9c2.png  \n",
            "  inflating: /root/train/window/mask/59454aa4-3405-44e0-95bf-3547037e5c0f.png  \n",
            "  inflating: /root/train/window/mask/5b266074-9960-4424-bb62-d8faadc5dd1a.png  \n",
            "  inflating: /root/train/window/mask/f5d32b09-aa93-4ca6-8ec7-4e56f79827e6.png  \n",
            "  inflating: /root/train/window/mask/7e000341-faa4-44d5-a213-45e9a7b74c74.png  \n",
            "  inflating: /root/train/window/mask/12e3a68b-4624-46b5-b135-8593159e21e5.png  \n",
            "  inflating: /root/train/window/mask/7b49559c-f54a-416e-8bb1-b62c17d517c4.png  \n",
            "  inflating: /root/train/window/mask/2eb34d1e-de5c-4c78-b47a-096db09ff40f.png  \n",
            "  inflating: /root/train/window/mask/4db16593-2ce6-4c1f-9985-b7a41aa0f2da.png  \n",
            "  inflating: /root/train/window/mask/ba6fe5db-8464-4bd4-9790-76ce86ef2975.png  \n",
            "  inflating: /root/train/window/mask/6dd39e76-7f4f-46aa-86dd-457dc94823b6.png  \n",
            "  inflating: /root/train/window/mask/790d29f9-6e5a-4394-b866-02d3f06ea52d.png  \n",
            "  inflating: /root/train/window/mask/2cd58e8b-f750-4e34-b0f2-c094b0c39e6b.png  \n",
            "  inflating: /root/train/window/mask/7ce98dbd-7a19-4093-a124-ac90ce6d1e51.png  \n",
            "  inflating: /root/train/window/mask/8a9cfe4c-b68b-425b-a17a-69ea22f10bc0.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Импорт необходимых библиотек\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Loss, Metric\n",
        "from ignite.engine import _prepare_batch\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import json\n",
        "import base64\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Callable, Tuple, Dict, Any, List, Sequence, Iterator, Optional\n",
        "from collections import defaultdict\n",
        "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn, FastRCNNPredictor"
      ],
      "metadata": {
        "id": "B8hpHLWYv9Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код для обучения нейросети - сегментация "
      ],
      "metadata": {
        "id": "qGts2McXePcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "## PLRC DATASET - класс и необходимые функции для датасета\n",
        "\n",
        "\n",
        "def get_color_map():\n",
        "    return {\n",
        "        \"wall\": 255,\n",
        "        \"window\": 255\n",
        "    }\n",
        "\n",
        "\n",
        "def tensor_from_rgb_image(image: np.ndarray) -> torch.Tensor:\n",
        "    image = np.moveaxis(image, -1, 0)\n",
        "    image = np.ascontiguousarray(image)\n",
        "    image = torch.from_numpy(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def tensor_from_mask_image(mask: np.ndarray) -> torch.Tensor:\n",
        "    if len(mask.shape) == 2:\n",
        "        mask = np.expand_dims(mask, -1)\n",
        "    return tensor_from_rgb_image(mask)\n",
        "\n",
        "\n",
        "class PLRCDataset(Dataset):\n",
        "    def __init__(self, image_folder, transform, start_index, end_index, mask_folder=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.mask_folder = mask_folder\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = PLRCDataset.parse_folder(self.image_folder, start_index, end_index)\n",
        "        self.color_map = get_color_map()\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_folder(path, start, end):\n",
        "        if path is None:\n",
        "            return []\n",
        "        images = glob.glob1(path,  '*.png')\n",
        "        images.sort()\n",
        "\n",
        "        return images[start:end]\n",
        "\n",
        "    @staticmethod\n",
        "    def load_image(path) -> np.array:\n",
        "        return cv2.imread(path, 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_mask(path) -> np.array:\n",
        "        return cv2.imread(path, 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_grayscale_mask_into_channels_by_color_map(mask, color_map) -> torch.Tensor:\n",
        "        masks = []\n",
        "\n",
        "        for i in color_map.values():\n",
        "            masks.append(mask == i)\n",
        "\n",
        "        return torch.cat(masks).float()\n",
        "\n",
        "    def mask_to_grayscale(self, masks) -> np.ndarray:\n",
        "        masks = masks.cpu().numpy()\n",
        "\n",
        "        colors_by_index = list(self.color_map.values())\n",
        "        img = np.zeros(masks.shape[1:], dtype=np.uint8)\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            img[masks[i] == 1] = colors_by_index[i]\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.images[index]\n",
        "        image_path = os.path.join(self.image_folder, image_name)\n",
        "\n",
        "        image = PLRCDataset.load_image(image_path)\n",
        "\n",
        "        if self.mask_folder is None:\n",
        "            # sample = self.transform(image=image)\n",
        "            # image = sample['image']\n",
        "            return image_name, tensor_from_mask_image(image).float() / 255.0\n",
        "\n",
        "        mask_path = os.path.join(self.mask_folder, image_name)\n",
        "        mask = PLRCDataset.load_mask(mask_path)\n",
        "\n",
        "        # sample = self.transform(image=image, mask=mask)\n",
        "        # image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        image = tensor_from_mask_image(image)\n",
        "        image = torch.cat([image, image, image])\n",
        "        mask = tensor_from_mask_image(mask)\n",
        "\n",
        "        mask = PLRCDataset.split_grayscale_mask_into_channels_by_color_map(mask, self.color_map)\n",
        "\n",
        "        return image.float() / 255.0, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "## PLRC UTILS = утилиты для подсчета\n",
        "\n",
        "\n",
        "def get_training_augmentation():\n",
        "    return A.Compose([\n",
        "        # A.RandomCrop(height=256, width=256, p=1),\n",
        "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    ], p=1)\n",
        "\n",
        "\n",
        "def get_test_augmentation():\n",
        "    return A.Compose([\n",
        "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    ], p=1)\n",
        "\n",
        "\n",
        "def get_data_loader(path, batch_size, n_processes, start_index, end_index, shuffle=True):\n",
        "    image_path = os.path.join(path, 'image')\n",
        "    mask_path = os.path.join(path, 'mask')\n",
        "\n",
        "    dataset = PLRCDataset(image_folder=image_path, mask_folder=mask_path, transform=get_training_augmentation(), start_index=start_index, end_index=end_index)\n",
        "\n",
        "    return DataLoader(dataset=dataset, batch_size=batch_size, drop_last=True, num_workers=n_processes, shuffle=shuffle)\n",
        "\n",
        "\n",
        "def get_train_validation_data_loaders(path, batch_size, n_processes, train_split):\n",
        "    files_count = len(os.listdir(os.path.join(path, 'image')))\n",
        "\n",
        "    train_dl = get_data_loader(path, batch_size, n_processes, shuffle=True, start_index=0, end_index=int(files_count*train_split))\n",
        "    test_dl = get_data_loader(path, batch_size, n_processes, shuffle=False, start_index=int(files_count*train_split), end_index=100)\n",
        "\n",
        "    return train_dl, test_dl\n",
        "\n",
        "\n",
        "## DATA LOSS - функции для подсчета метрик качества обучения нейросетей\n",
        "\n",
        "class BCESoftDiceLoss:\n",
        "    def __init__(self, dice_weight=0):\n",
        "        self.nll_loss = nn.BCEWithLogitsLoss()\n",
        "        self.dice_weight = dice_weight\n",
        "\n",
        "    @staticmethod\n",
        "    def soft_dice(predict, target):\n",
        "        eps = 1e-15\n",
        "        batch_size = target.size()[0]\n",
        "\n",
        "        dice_target = (target == 1).float().view(batch_size, -1)\n",
        "        dice_predict = torch.sigmoid(predict).view(batch_size, -1)\n",
        "\n",
        "        inter = torch.sum(dice_predict * dice_target) / batch_size\n",
        "        union = (torch.sum(dice_predict) + torch.sum(dice_target)) / batch_size + eps\n",
        "\n",
        "        return (2 * inter.float() + eps) / union.float()\n",
        "\n",
        "    def __call__(self, predict, target):\n",
        "        loss = (1.0 - self.dice_weight) * self.nll_loss(predict, target)\n",
        "\n",
        "        if self.dice_weight:\n",
        "            loss -= self.dice_weight * torch.log(BCESoftDiceLoss.soft_dice(predict, target))\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MultiClassBCESoftDiceLoss:\n",
        "    def __init__(self, dice_weight=0):\n",
        "        self.bce_soft_dice = BCESoftDiceLoss(dice_weight)\n",
        "\n",
        "    def __call__(self, predict, target):\n",
        "        classes = target.shape[1]\n",
        "        loss = predict.new_zeros(1)\n",
        "\n",
        "        for i in range(classes):\n",
        "            loss += self.bce_soft_dice(predict[:, i].unsqueeze(1), target[:, i].unsqueeze(1))\n",
        "\n",
        "        return loss[0] / float(classes)\n",
        "\n",
        "\n",
        "class MultiClassSoftDiceMetric(Metric):\n",
        "    def __init__(self):\n",
        "        super(MultiClassSoftDiceMetric, self).__init__()\n",
        "        self.general_loss = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.general_loss = 0\n",
        "\n",
        "    def update(self, output):\n",
        "        predict, target = output\n",
        "\n",
        "        classes = target.shape[1]\n",
        "        loss = predict.new_zeros(1)\n",
        "\n",
        "        for i in range(classes):\n",
        "            loss += BCESoftDiceLoss.soft_dice(predict[:, i].unsqueeze(1), target[:, i].unsqueeze(1))\n",
        "\n",
        "        self.general_loss = loss[0] / float(classes)\n",
        "\n",
        "    def compute(self):\n",
        "        return self.general_loss\n",
        "\n",
        "\n",
        "\n",
        "## Функции для обучения\n",
        "\n",
        "def load_trained_model(model, optimizer, model_path, optimizer_path):\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    optimizer.load_state_dict(torch.load(optimizer_path))\n",
        "    print('Load model from: ', model_path)\n",
        "    print('Load optimizer from: ', optimizer_path)\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, model_path, optimizer_path, postfix='_'):\n",
        "    torch.save(model.state_dict(), model_path + postfix)\n",
        "    torch.save(optimizer.state_dict(), optimizer_path + postfix)\n",
        "\n",
        "\n",
        "def log_image(image, prefix, epoch, step):\n",
        "    img = Image.fromarray(image)\n",
        "    image_name = \"%s_%s_%s.png\" % (epoch, step, prefix)\n",
        "    img.save(image_name)\n",
        "\n",
        "    os.remove(image_name)\n",
        "\n",
        "\n",
        "def run_test_model(model, evaluate_loader, epoch, device, step=10):\n",
        "    model.eval()\n",
        "    count_step = 0\n",
        "\n",
        "    for idx, batch in enumerate(evaluate_loader):\n",
        "        if count_step > step:\n",
        "            break\n",
        "\n",
        "        x, y = _prepare_batch(batch, device)\n",
        "\n",
        "        predict = model(x)\n",
        "        predict = torch.sigmoid(predict) > 0.2\n",
        "\n",
        "        count_step += len(x)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def run_train(dataset_path, batch_size, n_processes, model_path, optimizer_path, load_pre_model=False,\n",
        "              device='cpu', lr=0.0001, betas=(0.9, 0.99), weight_decay=0.0004, epochs=10,\n",
        "              log_interval=20, save_interval=2, train_split=0.7):\n",
        "\n",
        "    train_loader, evaluate_loader = get_train_validation_data_loaders(path=dataset_path, batch_size=batch_size,\n",
        "                                                                      n_processes=n_processes, train_split=train_split)\n",
        "    model = smp.FPN('resnet50', classes=24)\n",
        "\n",
        "    if device.startswith('cuda'):\n",
        "        if not torch.cuda.is_available():\n",
        "            raise ValueError('CUDA is not available')\n",
        "\n",
        "        model = model.to(device)\n",
        "        print('CUDA is used')\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "\n",
        "    if load_pre_model:\n",
        "        load_trained_model(model, optimizer, model_path, optimizer_path)\n",
        "\n",
        "    trainer = create_supervised_trainer(model, optimizer, MultiClassBCESoftDiceLoss(0.7), device=device)\n",
        "    evaluator = create_supervised_evaluator(model,\n",
        "                                            metrics={'dice': MultiClassSoftDiceMetric(),\n",
        "                                                     'nll': Loss(MultiClassBCESoftDiceLoss(0.7))},\n",
        "                                            device=device)\n",
        "\n",
        "    desc = \"ITERATION - loss: {:.2f}\"\n",
        "    pbar = None\n",
        "\n",
        "    @trainer.on(Events.EPOCH_STARTED)\n",
        "    def create_pbar(engine):\n",
        "        model.train()\n",
        "        nonlocal pbar\n",
        "        pbar = tqdm(\n",
        "            initial=0, leave=False, total=len(train_loader),\n",
        "            desc=desc.format(0)\n",
        "        )\n",
        "\n",
        "    @trainer.on(Events.EPOCH_COMPLETED)\n",
        "    def log_training_results(engine):\n",
        "        pbar.close()\n",
        "        evaluator.run(evaluate_loader)\n",
        "        metrics = evaluator.state.metrics\n",
        "        avg_dice = metrics['dice']\n",
        "        avg_nll = metrics['nll']\n",
        "\n",
        "\n",
        "        print(\"Training Results - Epoch: {}  Dice: {:.2f} Avg loss: {:.2f}\"\n",
        "              .format(engine.state.epoch, avg_dice, avg_nll))\n",
        "\n",
        "        if engine.state.epoch % save_interval == 0:\n",
        "            save_model(model, optimizer, model_path, optimizer_path, '_' + str(engine.state.epoch))\n",
        "            run_test_model(model, evaluate_loader, engine.state.epoch, device)\n",
        "\n",
        "    @trainer.on(Events.ITERATION_COMPLETED)\n",
        "    def log_training_loss(engine):\n",
        "\n",
        "        pbar.desc = desc.format(engine.state.output)\n",
        "        pbar.update()\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    trainer.run(train_loader, max_epochs=epochs)\n",
        "\n",
        "run_train(dataset_path=f\"{os.environ['HOME']}/train/wall\", batch_size=1, n_processes=0,\n",
        "          model_path='./model',\n",
        "          optimizer_path='./opt', device='cpu', epochs=2,\n",
        "          load_pre_model=False)"
      ],
      "metadata": {
        "id": "LFuViSg26sA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e4b35a-8e62-49c5-985c-926b6ccdd72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Results - Epoch: 1  Dice: 0.78 Avg loss: 0.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Results - Epoch: 2  Dice: 0.85 Avg loss: 0.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код для получения данных с сегментации"
      ],
      "metadata": {
        "id": "nV9nxL5wdar2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Необходимые вспомогательные функции\n",
        "def get_test_augmentation():\n",
        "    return A.Compose([\n",
        "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    ], p=1)\n",
        "\n",
        "def mask_to_grayscale(masks) -> np.ndarray:\n",
        "    masks = masks.cpu().numpy()\n",
        "\n",
        "    img = np.zeros(masks.shape[1:], dtype=np.uint8)\n",
        "\n",
        "    img[masks[0] == 1] = 255\n",
        "\n",
        "    return img\n",
        "\n",
        "def tensor_from_rgb_image(image: np.ndarray) -> torch.Tensor:\n",
        "    image = np.moveaxis(image, -1, 0)\n",
        "    image = np.ascontiguousarray(image)\n",
        "    image = torch.from_numpy(image)\n",
        "    return image\n",
        "\n",
        "def convert_image(img):\n",
        "    if img is None:\n",
        "        return\n",
        "    height, width, channels = img.shape\n",
        "    rect = (0, 0, 512, 512)\n",
        "    h = rect[3]\n",
        "    w = int(h * width / height)\n",
        "    if w > rect[2]:\n",
        "        w = rect[2]\n",
        "        h = int(height / width * w)\n",
        "    new_img = cv2.resize(img, (w, h), interpolation=cv2.INTER_AREA)\n",
        "    s = np.full((rect[3], rect[2], 3), np.uint8(255))\n",
        "    sy = int((rect[3] - h) / 2)\n",
        "    sx = int((rect[2] - w) / 2)\n",
        "    s[sy:sy + h, sx:sx + w] = new_img\n",
        "    return s\n",
        "\n",
        "## Загрузка модели\n",
        "model = smp.FPN(encoder_name='resnet50', classes=24)\n",
        "model.load_state_dict(torch.load(\"./model_2\", map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "##Загрузка тестового изображения\n",
        "image = cv2.imread(f\"{os.environ['HOME']}/train/test.png\")\n",
        "converted_image = convert_image(image)\n",
        "## Вывод исходного изображения\n",
        "plt.subplot(211)\n",
        "im1 = plt.imshow(converted_image)\n",
        "\n",
        "## Применение сегментации\n",
        "transform = get_test_augmentation()\n",
        "image = transform(image=converted_image)['image']\n",
        "\n",
        "image = tensor_from_rgb_image(image)\n",
        "image = image.view((1, 3, image.shape[1], image.shape[2]))\n",
        "with torch.no_grad():\n",
        "    predict = model(image)\n",
        "predict = torch.sigmoid(predict) > 0.2\n",
        "res = mask_to_grayscale(predict[0])\n",
        "\n",
        "## Вывод результирующего изображения\n",
        "plt.subplot(212)\n",
        "im1 = plt.imshow(res)\n"
      ],
      "metadata": {
        "id": "MrIseOYL6x94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "4b571a28-ff0d-465c-c9f6-e180aa275235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объявляем класс, используемый для обучения"
      ],
      "metadata": {
        "id": "ZnIyPcTgle1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionTrainer:\n",
        "\n",
        "    def __init__(self, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "                 device: str, metric_functions: List[Tuple[str, Callable]] = [],\n",
        "                 callbacks: List[Callable[[nn.Module, int], None]] = [], epoch_number: int = 0):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.metric_functions = metric_functions\n",
        "        self.callbacks = callbacks\n",
        "\n",
        "        self.epoch_number = epoch_number\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_batch(self, val_iterator: Iterator, eval_on_n_batches: int) -> Optional[Dict[str, float]]:\n",
        "        predictions = []\n",
        "        targets = []\n",
        "\n",
        "        for real_batch_number in range(eval_on_n_batches):\n",
        "            try:\n",
        "                xs, ys = next(val_iterator)\n",
        "\n",
        "                xs = torch.tensor(xs, device=self.device)\n",
        "                ys = [{key: torch.tensor(y[key], device=self.device) for key in y} for y in ys]\n",
        "            except StopIteration:\n",
        "                if real_batch_number == 0:\n",
        "                    return None\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            ys_pred = self.model.eval()(xs)\n",
        "\n",
        "            for y_pred, y_true in zip(ys_pred, ys):\n",
        "                predictions.append({key: y_pred[key].cpu().numpy() for key in y_pred})\n",
        "                targets.append({key: y_true[key].cpu().numpy() for key in y_true})\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        predictions_tensor = []\n",
        "        targets_tensor = []\n",
        "\n",
        "        for y_pred, y_true in zip(predictions, targets):\n",
        "            predictions_tensor.append({key: torch.tensor(y_pred[key], device=self.device) for key in y_pred})\n",
        "            targets_tensor.append({key: torch.tensor(y_true[key], device=self.device) for key in y_true})\n",
        "\n",
        "        for metric_name, metric_fn in self.metric_functions:\n",
        "            metrics[metric_name] = metric_fn(predictions, targets).item()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, val_loader: DataLoader, eval_on_n_batches: int = 1) -> Dict[str, float]:\n",
        "        metrics_sum = defaultdict(float)\n",
        "        num_batches = 0\n",
        "\n",
        "        val_iterator = iter(val_loader)\n",
        "\n",
        "        while True:\n",
        "            batch_metrics = self.evaluate_batch(val_iterator, eval_on_n_batches)\n",
        "\n",
        "            if batch_metrics is None:\n",
        "                break\n",
        "\n",
        "            for metric_name in batch_metrics:\n",
        "                metrics_sum[metric_name] += batch_metrics[metric_name]\n",
        "\n",
        "            num_batches += 1\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        for metric_name in metrics_sum:\n",
        "            metrics[metric_name] = metrics_sum[metric_name] / num_batches\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def fit_batch(self, train_iterator: Iterator, update_every_n_batches: int) -> Optional[Dict[str, float]]:\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        predictions = []\n",
        "        targets = []\n",
        "\n",
        "        for real_batch_number in range(update_every_n_batches):\n",
        "            try:\n",
        "                xs, ys = next(train_iterator)\n",
        "                xs = torch.tensor(xs, device=self.device)\n",
        "                ys = [{key: torch.tensor(y[key], device=self.device) for key in y} for y in ys]\n",
        "            except StopIteration:\n",
        "                if real_batch_number == 0:\n",
        "                    return None\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            loss = sum(self.model.train()(xs, ys).values())\n",
        "\n",
        "            (loss / update_every_n_batches).backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ys_pred = self.model.eval()(xs)\n",
        "\n",
        "                for y_pred, y_true in zip(ys_pred, ys):\n",
        "                    predictions.append({key: y_pred[key].cpu().numpy() for key in y_pred})\n",
        "                    targets.append({key: y_true[key].cpu().numpy() for key in y_true})\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        predictions_tensor = []\n",
        "        targets_tensor = []\n",
        "\n",
        "        for y_pred, y_true in zip(predictions, targets):\n",
        "            predictions_tensor.append({key: torch.tensor(y_pred[key], device=self.device) for key in y_pred})\n",
        "            targets_tensor.append({key: torch.tensor(y_true[key], device=self.device) for key in y_true})\n",
        "\n",
        "        for metric_name, metric_fn in self.metric_functions:\n",
        "            metrics[metric_name] = metric_fn(predictions, targets).item()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def fit_epoch(self, train_loader: DataLoader, update_every_n_batches: int = 1) -> Dict[str, float]:\n",
        "        metrics_sum = defaultdict(float)\n",
        "        num_batches = 0\n",
        "\n",
        "        train_iterator = iter(train_loader)\n",
        "\n",
        "        while True:\n",
        "            batch_metrics = self.fit_batch(train_iterator, update_every_n_batches)\n",
        "\n",
        "            if batch_metrics is None:\n",
        "                break\n",
        "\n",
        "            for metric_name in batch_metrics:\n",
        "                metrics_sum[metric_name] += batch_metrics[metric_name]\n",
        "\n",
        "            num_batches += 1\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        for metric_name in metrics_sum:\n",
        "            metrics[metric_name] = metrics_sum[metric_name] / num_batches\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def fit(self, train_loader: DataLoader, num_epochs: int,\n",
        "            val_loader: DataLoader = None, update_every_n_batches: int = 1\n",
        "            ) -> Dict[str, np.ndarray]:\n",
        "        summary = defaultdict(list)\n",
        "\n",
        "        def save_metrics(metrics: Dict[str, float], postfix: str = '') -> None:\n",
        "            nonlocal summary, self\n",
        "\n",
        "            for metric in metrics:\n",
        "                metric_name, metric_value = f'{metric}{postfix}', metrics[metric]\n",
        "\n",
        "                summary[metric_name].append(metric_value)\n",
        "\n",
        "                print(f\"{metric_name}: {metric_value}, Epoch: {self.epoch_number}\")\n",
        "\n",
        "        for _ in tqdm(range(num_epochs - self.epoch_number), initial=self.epoch_number, total=num_epochs):\n",
        "            self.epoch_number += 1\n",
        "\n",
        "            train_metrics = self.fit_epoch(train_loader, update_every_n_batches)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                save_metrics(train_metrics, postfix='_train')\n",
        "\n",
        "                if val_loader is not None:\n",
        "                    test_metrics = self.evaluate(val_loader)\n",
        "                    save_metrics(test_metrics, postfix='_test')\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for callback in self.callbacks:\n",
        "                        callback(self.model, self.epoch_number)\n",
        "\n",
        "        summary = {metric: np.array(summary[metric]) for metric in summary}\n",
        "\n",
        "        return summary"
      ],
      "metadata": {
        "id": "umyG-IbclmCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализация модели обучения и функций для подсчета метрик качества обучения нейросетей"
      ],
      "metadata": {
        "id": "klMp7gYWmDck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(num_classes: int,\n",
        "                     min_size: int, max_size: int,\n",
        "                     image_mean: Sequence[float], image_std: Sequence[float],\n",
        "                     device: str = None) -> torch.nn.Module:\n",
        "    pretrained_model = fasterrcnn_resnet50_fpn(pretrained=False, min_size=min_size, max_size=max_size,\n",
        "                                               image_mean=image_mean, image_std=image_std)\n",
        "    num_predictor_features = pretrained_model.roi_heads.box_head.fc7.out_features\n",
        "    pretrained_model.roi_heads.box_predictor = FastRCNNPredictor(num_predictor_features, num_classes)\n",
        "\n",
        "    return pretrained_model.to(device)\n",
        "\n",
        "\n",
        "def compute_iou(box1: torch.tensor, box2: torch.tensor) -> torch.tensor:\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    if x2 - x1 < 0 or y2 - y1 < 0:\n",
        "        return 0\n",
        "\n",
        "    intersection = (x2 - x1) * (y2 - y1)\n",
        "    sum_ = ((box1[2] - box1[0]) * (box1[3] - box1[1]) +\n",
        "            (box2[2] - box2[0]) * (box2[3] - box2[1]))\n",
        "\n",
        "    iou = intersection / (sum_ - intersection)\n",
        "\n",
        "    return iou.item()\n",
        "\n",
        "\n",
        "class MeanAveragePrecision:\n",
        "\n",
        "    def __init__(self, num_conf: int = 11):\n",
        "        self.num_conf = num_conf\n",
        "\n",
        "    @staticmethod\n",
        "    def is_true(box: torch.tensor, label: int,\n",
        "                y_true: Dict[str, torch.tensor],\n",
        "                iou_thresh: float = 0.5) -> bool:\n",
        "        num_true = len(y_true['boxes'])\n",
        "\n",
        "        for i in range(num_true):\n",
        "            if label == y_true['labels'][i]:\n",
        "                if compute_iou(box, y_true['boxes'][i]) > iou_thresh:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_map_given_conf(y_preds: List[Dict[str, torch.tensor]],\n",
        "                               y_trues: List[Dict[str, torch.tensor]],\n",
        "                               conf: float) -> float:\n",
        "        num_pos = defaultdict(int)\n",
        "        num_true_pos = defaultdict(int)\n",
        "\n",
        "        for y_pred, y_true in zip(y_preds, y_trues):\n",
        "\n",
        "            num_pred = len(y_pred['boxes'])\n",
        "\n",
        "            for i in range(num_pred):\n",
        "                if y_pred['scores'][i] > conf:\n",
        "                    label = y_pred['labels'][i]\n",
        "                    num_pos[label] += 1\n",
        "\n",
        "                    if MeanAveragePrecision.is_true(y_pred['boxes'][i], y_pred['labels'][i], y_true):\n",
        "                        num_true_pos[label] += 1\n",
        "\n",
        "        all_classes_sum = sum(num_true_pos[lbl] / num_pos[lbl] for lbl in num_pos)\n",
        "\n",
        "        try:\n",
        "            precision = all_classes_sum / len(num_pos)\n",
        "        except ZeroDivisionError:\n",
        "            precision = 0\n",
        "\n",
        "        return precision\n",
        "\n",
        "    def __call__(self, y_pred: List[Dict[str, torch.tensor]],\n",
        "                 y_true: List[Dict[str, torch.tensor]]) -> float:\n",
        "        assert len(y_pred) == len(y_true)\n",
        "\n",
        "        map_sum = 0\n",
        "\n",
        "        for conf in np.linspace(0, 1, self.num_conf):\n",
        "            map_sum += self.compute_map_given_conf(y_pred, y_true, conf)\n",
        "\n",
        "        return torch.tensor(map_sum / self.num_conf)\n",
        "\n",
        "\n",
        "def minimum_bounding_box(points: List[List[float]]) -> Tuple[float, float, float, float]:\n",
        "    x_min = min(p[0] for p in points)\n",
        "    y_min = min(p[1] for p in points)\n",
        "    x_max = max(p[0] for p in points)\n",
        "    y_max = max(p[1] for p in points)\n",
        "\n",
        "    return x_min, y_min, x_max, y_max\n"
      ],
      "metadata": {
        "id": "-0D06LoHmQP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Классы, для формирования датасетов(для обучения и для тестирования)"
      ],
      "metadata": {
        "id": "3VH_0TWXmYS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataRetriever:\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataset_path: str):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset_path = dataset_path\n",
        "        self.samples_paths = self.load_paths()\n",
        "        print(f'Found {len(self.samples_paths)} samples')\n",
        "\n",
        "    def remove_path(self, idx):\n",
        "        del self.samples_paths[idx]\n",
        "\n",
        "    def load_paths(self) -> List[str]:\n",
        "        samples_paths = []\n",
        "\n",
        "        for object_ in tqdm(os.listdir(self.dataset_path)):\n",
        "            ext = object_.split(\".\")[-1]\n",
        "\n",
        "            if ext != 'json':\n",
        "                continue\n",
        "\n",
        "            samples_paths.append(self.dataset_path + \"/\" + object_)\n",
        "\n",
        "        return samples_paths\n",
        "\n",
        "    def num_paths(self) -> int:\n",
        "        return len(self.samples_paths)\n",
        "\n",
        "\n",
        "class PLRCDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_retriever,\n",
        "                 class_ids: Dict,\n",
        "                 transform: A.BasicTransform = None,\n",
        "                 remove_unannotated: bool = False):\n",
        "        self.data_retriever = data_retriever\n",
        "\n",
        "        self.class_ids = class_ids\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        if remove_unannotated:\n",
        "            print(\"Removing unannotated samples\")\n",
        "            num_removed = self.remove_unannotated_paths()\n",
        "            print(f\"Removed {num_removed} unannotated samples\")\n",
        "\n",
        "    def make_layout(self, shapes: List[Dict], image_size: Tuple[int, int]) -> Dict[str, np.ndarray]:\n",
        "        shapes = self.select_shapes(shapes)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for shape in shapes:\n",
        "            if len(shape['points']) < 2:\n",
        "                continue\n",
        "\n",
        "            label = shape['label']\n",
        "\n",
        "            if label not in self.class_ids:\n",
        "                continue\n",
        "\n",
        "            x1, y1, x2, y2 = minimum_bounding_box(shape['points'])\n",
        "\n",
        "            x1 = max(x1, 0)\n",
        "            y1 = max(y1, 0)\n",
        "            x2 = min(x2, image_size[1] - 1)\n",
        "            y2 = min(y2, image_size[0] - 1)\n",
        "\n",
        "            if x1 == x2 or y1 == y2:\n",
        "                continue\n",
        "\n",
        "            boxes.append((x1, y1, x2, y2))\n",
        "            labels.append(self.class_ids[label])\n",
        "\n",
        "        if len(boxes) > 0:\n",
        "            boxes = np.array(boxes)\n",
        "            labels = np.array(labels, dtype=np.int64)\n",
        "        else:\n",
        "            boxes = np.empty((0, 4))\n",
        "            labels = np.empty(0, dtype=np.int64)\n",
        "\n",
        "        return {'boxes': boxes, 'labels': labels}\n",
        "\n",
        "    @staticmethod\n",
        "    def select_shapes(shapes: List[Dict]) -> List[Dict]:\n",
        "        good_shapes = []\n",
        "\n",
        "        for shape in shapes:\n",
        "            if shape['shape_type'] not in {'polygon', 'rectangle'} or len(shape['points']) < 2:\n",
        "                continue\n",
        "\n",
        "            good_shapes.append(shape)\n",
        "\n",
        "        return good_shapes\n",
        "\n",
        "    def remove_unannotated_paths(self) -> int:\n",
        "        counter = 0\n",
        "\n",
        "        for i in tqdm(range(len(self) - 1, -1, -1)):\n",
        "            _, y = self[i]\n",
        "            if len(y['boxes']) == 0:\n",
        "                self.data_retriever.remove_path(i)\n",
        "                counter += 1\n",
        "\n",
        "        return counter\n",
        "\n",
        "    @staticmethod\n",
        "    def decode_image(encoded_image: str) -> np.ndarray:\n",
        "        bytearray_ = np.asarray(bytearray(base64.b64decode(encoded_image)), dtype=np.uint8)\n",
        "        return cv2.imdecode(bytearray_, cv2.IMREAD_COLOR).astype(np.float32) / 255\n",
        "\n",
        "    def read_sample(self, json_path) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        with open(json_path, 'r') as f:\n",
        "            json_contents = json.load(f)\n",
        "\n",
        "        image = self.decode_image(json_contents['imageData'])\n",
        "        layout = self.make_layout(json_contents['shapes'], image.shape[:2])\n",
        "\n",
        "        return image, layout\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_path = self.data_retriever.samples_paths[idx]\n",
        "\n",
        "        x, y = self.read_sample(sample_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=x, bboxes=y['boxes'], labels=y['labels'])\n",
        "            x, y['boxes'], y['labels'] = transformed['image'], transformed['bboxes'], transformed['labels']\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_retriever.num_paths()\n",
        "\n",
        "\n",
        "class DatasetPart(Dataset):\n",
        "\n",
        "    def __init__(self, dataset: Dataset,\n",
        "                 indices: np.ndarray,\n",
        "                 transform: A.BasicTransform = None):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Any:\n",
        "        x, y = self.dataset[self.indices[idx]]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=x, bboxes=y['boxes'], labels=y['labels'])\n",
        "            x, y['boxes'], y['labels'] = transformed['image'], transformed['bboxes'], transformed['labels']\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n"
      ],
      "metadata": {
        "id": "esvFKW94mdaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объявляем конфиг для object_detection"
      ],
      "metadata": {
        "id": "HfkIFx1ylP7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "  \"dataset_path\": \"train/object_detection\",\n",
        "  \"model_log_interval\": 1,\n",
        "  \"data\": {\n",
        "    \"class_ids\": {\n",
        "      \"door\": 1\n",
        "    }\n",
        "  },\n",
        "  \"model\": {\n",
        "    \"image_size\": 512\n",
        "  },\n",
        "  \"training\": {\n",
        "    \"num_epochs\": 2,\n",
        "    \"batch_size\": 1,\n",
        "    \"update_every_n_batches\": 1,\n",
        "    \"device\": \"cpu\",\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"num_workers\": 0\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "MCjuuZnqlXs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код обучения"
      ],
      "metadata": {
        "id": "AAss44LcmkvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_transforms(config: Dict[str, Any]) -> Dict[str, Callable]:\n",
        "    size = config['model']['image_size']\n",
        "\n",
        "    train_list = [A.LongestMaxSize(size),\n",
        "                  A.PadIfNeeded(size, size, border_mode=cv2.BORDER_REPLICATE),\n",
        "                  ToTensorV2()]\n",
        "    eval_list = [A.LongestMaxSize(size),\n",
        "                 A.PadIfNeeded(size, size, border_mode=cv2.BORDER_REPLICATE),\n",
        "                 ToTensorV2()]\n",
        "\n",
        "    return {'train': A.Compose(train_list, bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
        "            'test': A.Compose(eval_list, bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))}\n",
        "\n",
        "\n",
        "def make_datasets(transforms, config: Dict[str, Any]) -> Dict[str, Dataset]:\n",
        "    all_data_retriever = DataRetriever(f\"{os.environ['HOME']}/{config['dataset_path']}\")\n",
        "\n",
        "    all_data_dataset = PLRCDataset(all_data_retriever, config['data']['class_ids'], remove_unannotated=True)\n",
        "\n",
        "    train_indices, test_indices = train_test_split(range(len(all_data_dataset)), test_size=0.3)\n",
        "\n",
        "    return {'train': DatasetPart(all_data_dataset, train_indices, transform=transforms['train']),\n",
        "            'test': DatasetPart(all_data_dataset, test_indices, transform=transforms['test'])}\n",
        "\n",
        "\n",
        "def make_loaders(datasets, config: Dict[str, Any]):\n",
        "    def collate_fn(samples):\n",
        "        xs, ys = list(zip(*samples))\n",
        "\n",
        "        for y in ys:\n",
        "            if len(y['boxes']) == 0:\n",
        "                y['boxes'] = np.empty((0, 4), dtype=np.float32)\n",
        "            else:\n",
        "                y['boxes'] = np.array(y['boxes'], dtype=np.float32)\n",
        "\n",
        "            y['labels'] = np.array(y['labels'], dtype=np.int64)\n",
        "\n",
        "        return torch.stack(xs), ys\n",
        "\n",
        "    loaders = {}\n",
        "\n",
        "    for name in datasets:\n",
        "        loaders[name] = DataLoader(datasets[name], config['training']['batch_size'],\n",
        "                                   num_workers=config['training']['num_workers'],\n",
        "                                   shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    return loaders\n",
        "\n",
        "\n",
        "def make_save_model_callback(log_interval: int):\n",
        "    def save_model(model, epoch):\n",
        "        if epoch % log_interval != 0:\n",
        "            return\n",
        "        torch.save(model.state_dict(), f\"./model_obj_{epoch}\")\n",
        "\n",
        "    return save_model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    transforms = make_transforms(config)\n",
        "    datasets = make_datasets(transforms, config)\n",
        "    data_loaders = make_loaders(datasets, config)\n",
        "    \n",
        "    model = initialize_model(1 + max(config['data']['class_ids'].values()),\n",
        "                             config['model']['image_size'], config['model']['image_size'], [0, 0, 0], [1, 1, 1],\n",
        "                             config['training']['device'])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), config['training']['learning_rate'])\n",
        "\n",
        "    metrics = [('mAP', MeanAveragePrecision())]\n",
        "\n",
        "    trainer = DetectionTrainer(model, optimizer, metric_functions=metrics,\n",
        "                               callbacks=[make_save_model_callback(config['model_log_interval'])],\n",
        "                               device=config['training']['device'])\n",
        "\n",
        "    trainer.fit(data_loaders['train'], config['training']['num_epochs'], val_loader=data_loaders['test'])"
      ],
      "metadata": {
        "id": "ieEUG7eQmmQr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "1f6a44d536d84731a82ac6871085dbd2",
            "f67da5c894bd4941b4c2b693dd067636",
            "fb108adb8d00410e980b7d1e66b44262",
            "df2ec3c39e924d2394020b525b598536",
            "bc2bd476a44d427fb03d86ac038a2d69",
            "b77f1d736361484498345e29c41962b7",
            "0b8a886270204320ab131200ffd3878f",
            "b0cb6780a3324eaab6415a32a45d4f69",
            "53485f63532b4813a25e7d5ba9fa77e1",
            "39ac5c507a6549ee96e4aaf67908ddd1",
            "12f2787c151d43d2a15c3aacafaaac97"
          ]
        },
        "outputId": "e2a17457-2e02-4869-8700-1bd7623bb713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 464485.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 samples\n",
            "Removing unannotated samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 54.79it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 21 unannotated samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f6a44d536d84731a82ac6871085dbd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mAP_train: 0.0024517346321689813, Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mAP_test: 0.005966106415144168, Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [12:37<12:37, 757.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mAP_train: 0.016041485197969118, Epoch: 2\n",
            "mAP_test: 0.010372887016274035, Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [23:30<00:00, 705.18s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод конечного результата"
      ],
      "metadata": {
        "id": "ElCb7TxqoOYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def draw_with_boxes(image: np.ndarray,\n",
        "                    preds: Dict[str, np.ndarray] = None,\n",
        "                    conf_threshold=0.5) -> np.ndarray:\n",
        "\n",
        "    image = image.copy()\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
        "    font_scale = 0.7\n",
        "    line_type = 1\n",
        "    label_padding = 5\n",
        "\n",
        "    num_boxes = len(preds['boxes'])\n",
        "\n",
        "    for i in range(num_boxes):\n",
        "        if preds['scores'][i] < conf_threshold:\n",
        "            continue\n",
        "        x_min, y_min, x_max, y_max = preds['boxes'][i]\n",
        "\n",
        "        cv2.rectangle(img=image,\n",
        "                      pt1=(int(x_min), int(y_min)),\n",
        "                      pt2=(int(x_max), int(y_max)),\n",
        "                      color=(0, 0, 255),\n",
        "                      thickness=-1)\n",
        "\n",
        "    return image\n",
        "\n",
        "##Загрузка тестового изображения\n",
        "image = cv2.imread(f\"{os.environ['HOME']}/train/test.png\").astype(np.float32) / 255\n",
        "\n",
        "## Вывод исходного изображения\n",
        "plt.subplot(211)\n",
        "im1 = plt.imshow(image)\n",
        "\n",
        "## Применение модели\n",
        "class_ids = config['data']['class_ids']\n",
        "reverse_classes_map = {v: k for k, v in class_ids.items()}\n",
        "bytearray_ = np.asarray(bytearray(image), dtype=np.uint8)\n",
        "x = image\n",
        "transform = ToTensorV2()\n",
        "model = initialize_model(1 + max(config['data']['class_ids'].values()),\n",
        "                             config['model']['image_size'], config['model']['image_size'], [0, 0, 0], [1, 1, 1],\n",
        "                             config['training']['device'])\n",
        "model.load_state_dict(torch.load(\"./model_obj_2\"))\n",
        "x = transform(image=x)['image']\n",
        "with torch.no_grad():\n",
        "  y_pred = model.eval()([torch.tensor(x, device=config['training']['device'])])[0]\n",
        "  y_pred = {key: y_pred[key].cpu().numpy() for key in y_pred}\n",
        "  y_pred['labels'] = np.array([reverse_classes_map[label] for label in y_pred['labels']])\n",
        "\n",
        "  x = torch.moveaxis(x, 0, -1).cpu().numpy()\n",
        "  x = np.ascontiguousarray(255 * x, dtype=np.uint8)\n",
        "\n",
        "image_with_boxes = draw_with_boxes(x, y_pred)\n",
        "\n",
        "## Вывод реузультирующего изображения\n",
        "plt.subplot(212)\n",
        "im1 = plt.imshow(image_with_boxes)\n"
      ],
      "metadata": {
        "id": "eQn_ToywouNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "e2c37fcc-e527-4eef-e6da-311f0e73abaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}